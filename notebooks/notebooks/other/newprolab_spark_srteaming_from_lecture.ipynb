{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Structured Streaming I\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Общие сведения\n",
    "+ Rate streaming\n",
    "+ File streaming\n",
    "+ Kafka streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общие сведения\n",
    "\n",
    "Системы поточной обработки данных:\n",
    "- работают с непрерывным потоком данных\n",
    "- нужно хранить состояние стрима\n",
    "- результат обработки быстро появляется в целевой системе\n",
    "- должны проектироваться с учетом требований к высокой доступности\n",
    "- важная скорость обработки данных и время зажержки (лаг)\n",
    "\n",
    "### Примеры систем поточной обработки данных\n",
    "\n",
    "#### Карточный процессинг\n",
    "- нельзя терять платежи\n",
    "- нельзя дублировать платежи\n",
    "- простой сервиса недопустим\n",
    "- максимальное время задержки ~ 1 сек\n",
    "- небольшой поток событий\n",
    "- OLTP\n",
    "\n",
    "#### Обработка логов безопасности\n",
    "- потеря единичных событий допустима\n",
    "- дублирование единичных событий допустимо\n",
    "- простой сервиса допустим\n",
    "- максимальное время задержки ~ 1 час\n",
    "- большой поток событий\n",
    "- OLAP\n",
    "\n",
    "### Виды стриминг систем\n",
    "\n",
    "#### Real-time streaming\n",
    "- низкие задержки на обработку\n",
    "- низкая пропускная способность\n",
    "- подходят для критичных систем\n",
    "- пособытийная обработка\n",
    "- OLTP\n",
    "- exactly once consistency (нет потери данных и нет дубликатов)\n",
    "\n",
    "#### Micro batch streaming\n",
    "- высокие задержки\n",
    "- высокая пропускная способность\n",
    "- не подходят для критичных систем\n",
    "- обработка батчами\n",
    "- OLAP\n",
    "- at least once consistency (во время сбоев могут возникать дубликаты)\n",
    "\n",
    "### Выводы:\n",
    "+ Существуют два типа систем поточной обработки данных - real-time и micro-batch\n",
    "+ Spark Structured Streaming является micro-batch системой\n",
    "+ При работе с большими данными обычно пропускная способность важнее, чем время задержки\n",
    "\n",
    "\n",
    "## Rate streaming\n",
    "\n",
    "Самый простой способ создать стрим - использовать `rate` источник. Созданный DF является streaming, о чем нам говорит метод создания `readStream` и атрибут `isStreaming`. `rate` хорошо подходит для тестирования приложений, когда нет возможности подключится к потоку реальных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sdf = [timestamp: timestamp, value: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf = spark.readStream.format(\"rate\").load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У `sdf`, как и у любого DF, есть схема и план выполнения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@47106b8f, rate, [timestamp#4, value#5L]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "timestamp: timestamp, value: bigint\n",
      "StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@47106b8f, rate, [timestamp#4, value#5L]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@47106b8f, rate, [timestamp#4, value#5L]\n",
      "\n",
      "== Physical Plan ==\n",
      "StreamingRelation rate, [timestamp#4, value#5L]\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema\n",
    "sdf.explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличии от обычных DF, у `sdf` нет таких методов, как `show`, `collect`, `take`. Для них также недоступен Dataset API. Поэтому для того, чтобы посмотреть их содержимое, мы должны использовать `console` синк и создать `StreamingQuery`. Процессинг начинается только после вызова метода `start`. `trigger` позволяет настроить, как часто стрим будет читать новые данные и обрабатывать их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSink: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createConsoleSink(df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sink = org.apache.spark.sql.streaming.DataStreamWriter@55186f65\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.streaming.DataStreamWriter@55186f65"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sink = createConsoleSink(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@65946306\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@65946306"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:29:04.976|0    |\n",
      "|2020-06-15 19:29:05.976|1    |\n",
      "|2020-06-15 19:29:06.976|2    |\n",
      "|2020-06-15 19:29:07.976|3    |\n",
      "|2020-06-15 19:29:08.976|4    |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:29:09.976|5    |\n",
      "|2020-06-15 19:29:10.976|6    |\n",
      "|2020-06-15 19:29:11.976|7    |\n",
      "|2020-06-15 19:29:12.976|8    |\n",
      "|2020-06-15 19:29:13.976|9    |\n",
      "|2020-06-15 19:29:14.976|10   |\n",
      "|2020-06-15 19:29:15.976|11   |\n",
      "|2020-06-15 19:29:16.976|12   |\n",
      "|2020-06-15 19:29:17.976|13   |\n",
      "|2020-06-15 19:29:18.976|14   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:29:19.976|15   |\n",
      "|2020-06-15 19:29:20.976|16   |\n",
      "|2020-06-15 19:29:21.976|17   |\n",
      "|2020-06-15 19:29:22.976|18   |\n",
      "|2020-06-15 19:29:23.976|19   |\n",
      "|2020-06-15 19:29:24.976|20   |\n",
      "|2020-06-15 19:29:25.976|21   |\n",
      "|2020-06-15 19:29:26.976|22   |\n",
      "|2020-06-15 19:29:27.976|23   |\n",
      "|2020-06-15 19:29:28.976|24   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:29:29.976|25   |\n",
      "|2020-06-15 19:29:30.976|26   |\n",
      "|2020-06-15 19:29:31.976|27   |\n",
      "|2020-06-15 19:29:32.976|28   |\n",
      "|2020-06-15 19:29:33.976|29   |\n",
      "|2020-06-15 19:29:34.976|30   |\n",
      "|2020-06-15 19:29:35.976|31   |\n",
      "|2020-06-15 19:29:36.976|32   |\n",
      "|2020-06-15 19:29:37.976|33   |\n",
      "|2020-06-15 19:29:38.976|34   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:29:39.976|35   |\n",
      "|2020-06-15 19:29:40.976|36   |\n",
      "|2020-06-15 19:29:41.976|37   |\n",
      "|2020-06-15 19:29:42.976|38   |\n",
      "|2020-06-15 19:29:43.976|39   |\n",
      "|2020-06-15 19:29:44.976|40   |\n",
      "|2020-06-15 19:29:45.976|41   |\n",
      "|2020-06-15 19:29:46.976|42   |\n",
      "|2020-06-15 19:29:47.976|43   |\n",
      "|2020-06-15 19:29:48.976|44   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:29:49.976|45   |\n",
      "|2020-06-15 19:29:50.976|46   |\n",
      "|2020-06-15 19:29:51.976|47   |\n",
      "|2020-06-15 19:29:52.976|48   |\n",
      "|2020-06-15 19:29:53.976|49   |\n",
      "|2020-06-15 19:29:54.976|50   |\n",
      "|2020-06-15 19:29:55.976|51   |\n",
      "|2020-06-15 19:29:56.976|52   |\n",
      "|2020-06-15 19:29:57.976|53   |\n",
      "|2020-06-15 19:29:58.976|54   |\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sq = sink.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы остановить DF, можно вызвать метод `stop` к `sdf`, либо получить список всех streming DF и остановить их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "killAll: ()Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "def killAll() = {\n",
    "    SparkSession\n",
    "        .active\n",
    "        .streams\n",
    "        .active\n",
    "        .foreach { x =>\n",
    "                    val desc = x.lastProgress.sources.head.description\n",
    "                    x.stop\n",
    "                    println(s\"Stopped ${desc}\")\n",
    "        }               \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "killAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим стрим, выполняющий запись в `parquet` файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createParquetSink: (df: org.apache.spark.sql.DataFrame, fileName: String)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createParquetSink(df: DataFrame, \n",
    "                      fileName: String) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", s\"datasets/$fileName\")\n",
    "    .option(\"checkpointLocation\", s\"chk/$fileName\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sink = org.apache.spark.sql.streaming.DataStreamWriter@58e88edf\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.streaming.DataStreamWriter@58e88edf"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sink = createParquetSink(sdf, \"s1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5766e1f5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5766e1f5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sq = sink.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что стрим пишется в файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"total 880\n",
       "drwxr-xr-x    9 t3nq  staff   288B Jun 15 19:40 _spark_metadata\n",
       "-rw-r--r--    1 t3nq  staff   756B Jun 15 19:40 part-00005-2ea3ca91-ecae-4c9f-8b66-185ffe02ebfe-c000.snappy.parquet\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun 15 19:40 .part-00005-2ea3ca91-ecae-4c9f-8b66-185ffe02ebfe-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff   756B Jun 15 19:40 part-00002-6e470e5c-34b0-4acd-ac49-3e103bc17c39-c000.snappy.parquet\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun 15 19:40 .part-00002-6e470e5c-34b0-4acd-ac49-3e103bc17c39-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff   756B Jun 15 19:40 part-00001-e84ae4ad-fb6d-46ea-a5bc-a80262676389-c000.snappy.parquet\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun 15 19:40 .part-00001-e84ae4ad-fb6d-46ea-a5bc-a8026267638...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\"ls -alht datasets/s1.parquet\".!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем файл с помощью Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:39:05.756|0    |\n",
      "|2020-06-15 19:39:06.756|1    |\n",
      "|2020-06-15 19:39:07.756|2    |\n",
      "|2020-06-15 19:39:08.756|3    |\n",
      "|2020-06-15 19:39:09.756|4    |\n",
      "+-----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rates = [timestamp: timestamp, value: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rates = spark.read.parquet(\"datasets/s1.parquet\")\n",
    "println(rates.count)\n",
    "rates.printSchema\n",
    "rates.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параллельно внутри одного Spark приложения может работать несколько стримов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(http://192.168.88.253:4040)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "consoleSink = org.apache.spark.sql.streaming.DataStreamWriter@f873134\n",
       "consoleSq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3eee817b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3eee817b"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:43:24.088|0    |\n",
      "|2020-06-15 19:43:25.088|1    |\n",
      "|2020-06-15 19:43:26.088|2    |\n",
      "|2020-06-15 19:43:27.088|3    |\n",
      "|2020-06-15 19:43:28.088|4    |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:43:29.088|5    |\n",
      "|2020-06-15 19:43:30.088|6    |\n",
      "|2020-06-15 19:43:31.088|7    |\n",
      "|2020-06-15 19:43:32.088|8    |\n",
      "|2020-06-15 19:43:33.088|9    |\n",
      "|2020-06-15 19:43:34.088|10   |\n",
      "|2020-06-15 19:43:35.088|11   |\n",
      "|2020-06-15 19:43:36.088|12   |\n",
      "|2020-06-15 19:43:37.088|13   |\n",
      "|2020-06-15 19:43:38.088|14   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:43:39.088|15   |\n",
      "|2020-06-15 19:43:40.088|16   |\n",
      "|2020-06-15 19:43:41.088|17   |\n",
      "|2020-06-15 19:43:42.088|18   |\n",
      "|2020-06-15 19:43:43.088|19   |\n",
      "|2020-06-15 19:43:44.088|20   |\n",
      "|2020-06-15 19:43:45.088|21   |\n",
      "|2020-06-15 19:43:46.088|22   |\n",
      "|2020-06-15 19:43:47.088|23   |\n",
      "|2020-06-15 19:43:48.088|24   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:43:49.088|25   |\n",
      "|2020-06-15 19:43:50.088|26   |\n",
      "|2020-06-15 19:43:51.088|27   |\n",
      "|2020-06-15 19:43:52.088|28   |\n",
      "|2020-06-15 19:43:53.088|29   |\n",
      "|2020-06-15 19:43:54.088|30   |\n",
      "|2020-06-15 19:43:55.088|31   |\n",
      "|2020-06-15 19:43:56.088|32   |\n",
      "|2020-06-15 19:43:57.088|33   |\n",
      "|2020-06-15 19:43:58.088|34   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:43:59.088|35   |\n",
      "|2020-06-15 19:44:00.088|36   |\n",
      "|2020-06-15 19:44:01.088|37   |\n",
      "|2020-06-15 19:44:02.088|38   |\n",
      "|2020-06-15 19:44:03.088|39   |\n",
      "|2020-06-15 19:44:04.088|40   |\n",
      "|2020-06-15 19:44:05.088|41   |\n",
      "|2020-06-15 19:44:06.088|42   |\n",
      "|2020-06-15 19:44:07.088|43   |\n",
      "|2020-06-15 19:44:08.088|44   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:44:09.088|45   |\n",
      "|2020-06-15 19:44:10.088|46   |\n",
      "|2020-06-15 19:44:11.088|47   |\n",
      "|2020-06-15 19:44:12.088|48   |\n",
      "|2020-06-15 19:44:13.088|49   |\n",
      "|2020-06-15 19:44:14.088|50   |\n",
      "|2020-06-15 19:44:15.088|51   |\n",
      "|2020-06-15 19:44:16.088|52   |\n",
      "|2020-06-15 19:44:17.088|53   |\n",
      "|2020-06-15 19:44:18.088|54   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:44:19.088|55   |\n",
      "|2020-06-15 19:44:20.088|56   |\n",
      "|2020-06-15 19:44:21.088|57   |\n",
      "|2020-06-15 19:44:22.088|58   |\n",
      "|2020-06-15 19:44:23.088|59   |\n",
      "|2020-06-15 19:44:24.088|60   |\n",
      "|2020-06-15 19:44:25.088|61   |\n",
      "|2020-06-15 19:44:26.088|62   |\n",
      "|2020-06-15 19:44:27.088|63   |\n",
      "|2020-06-15 19:44:28.088|64   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:44:29.088|65   |\n",
      "|2020-06-15 19:44:30.088|66   |\n",
      "|2020-06-15 19:44:31.088|67   |\n",
      "|2020-06-15 19:44:32.088|68   |\n",
      "|2020-06-15 19:44:33.088|69   |\n",
      "|2020-06-15 19:44:34.088|70   |\n",
      "|2020-06-15 19:44:35.088|71   |\n",
      "|2020-06-15 19:44:36.088|72   |\n",
      "|2020-06-15 19:44:37.088|73   |\n",
      "|2020-06-15 19:44:38.088|74   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:44:39.088|75   |\n",
      "|2020-06-15 19:44:40.088|76   |\n",
      "|2020-06-15 19:44:41.088|77   |\n",
      "|2020-06-15 19:44:42.088|78   |\n",
      "|2020-06-15 19:44:43.088|79   |\n",
      "|2020-06-15 19:44:44.088|80   |\n",
      "|2020-06-15 19:44:45.088|81   |\n",
      "|2020-06-15 19:44:46.088|82   |\n",
      "|2020-06-15 19:44:47.088|83   |\n",
      "|2020-06-15 19:44:48.088|84   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:44:49.088|85   |\n",
      "|2020-06-15 19:44:50.088|86   |\n",
      "|2020-06-15 19:44:51.088|87   |\n",
      "|2020-06-15 19:44:52.088|88   |\n",
      "|2020-06-15 19:44:53.088|89   |\n",
      "|2020-06-15 19:44:54.088|90   |\n",
      "|2020-06-15 19:44:55.088|91   |\n",
      "|2020-06-15 19:44:56.088|92   |\n",
      "|2020-06-15 19:44:57.088|93   |\n",
      "|2020-06-15 19:44:58.088|94   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-15 19:44:59.088|95   |\n",
      "|2020-06-15 19:45:00.088|96   |\n",
      "|2020-06-15 19:45:01.088|97   |\n",
      "|2020-06-15 19:45:02.088|98   |\n",
      "|2020-06-15 19:45:03.088|99   |\n",
      "|2020-06-15 19:45:04.088|100  |\n",
      "|2020-06-15 19:45:05.088|101  |\n",
      "|2020-06-15 19:45:06.088|102  |\n",
      "|2020-06-15 19:45:07.088|103  |\n",
      "|2020-06-15 19:45:08.088|104  |\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val consoleSink = createConsoleSink(sdf)\n",
    "val consoleSq = consoleSink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n",
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая добавляет к нашей колонке случайный `ident` аэропорта из датасета [Airport Codes](https://datahub.io/core/airport-codes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"datasets/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------\n",
      " value | ident,type,name,elevation_ft,continent,iso_country,iso_region,municipality,gps_code,iata_code,local_code,coordinates \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------\n",
      " value | 00A,heliport,Total Rf Heliport,11,NA,US,US-PA,Bensalem,00A,,00A,\"-74.93360137939453, 40.07080078125\"                 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text(\"datasets/airport-codes.csv\").show(2, 200, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idents = Array(00A, 00AA, 00AK, 00AL, 00AR, 00AS, 00AZ, 00CA, 00CL, 00CN, 00CO, 00FA, 00FD, 00FL, 00GA, 00GE, 00HI, 00ID, 00IG, 00II, 00IL, 00IN, 00IS, 00KS, 00KY, 00LA, 00LL, 00LS, 00MD, 00MI, 00MN, 00MO, 00MT, 00N, 00NC, 00NJ, 00NK, 00NY, 00OH, 00OI, 00OK, 00OR, 00PA, 00PN, 00PS, 00S, 00SC, 00SD, 00TA, 00TE, 00TN, 00TS, 00TX, 00UT, 00VA, 00VI, 00W, 00WA, 00WI, 00WN, 00WV, 00WY, 00XS, 01A, 01AK, 01AL, 01AR, 01AZ, 01C, 01CA, 01CL, 01CN, 01CO, 01CT, 01FA, 01FD, 01FL, 01GA, 01GE, 01IA, 01ID, 01II, 01IL, 01IN, 01IS, 01J, 01K, 01KS, 01KY, 01LA, 01LL, 01LS, 01MA, 01MD, 01ME, 01MI, 01MN, 01MO, 01MT, 01NC, 01NE, 01NH, 01NJ, 01NM, 01NV, 01NY, 01OI, 01OK, 01OR, 01PA, 01PN, 01PS, 01SC, 01TA, 01TE, 01TN, 01TS, 01TX, 01U, 01UT, 01VA, 01WA, 01WI, 01WN, 01WT, 01WY, 01XA, 01XS, 02AK, 02...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(00A, 00AA, 00AK, 00AL, 00AR, 00AS, 00AZ, 00CA, 00CL, 00CN, 00CO, 00FA, 00FD, 00FL, 00GA, 00GE, 00HI, 00ID, 00IG, 00II, 00IL, 00IN, 00IS, 00KS, 00KY, 00LA, 00LL, 00LS, 00MD, 00MI, 00MN, 00MO, 00MT, 00N, 00NC, 00NJ, 00NK, 00NY, 00OH, 00OI, 00OK, 00OR, 00PA, 00PN, 00PS, 00S, 00SC, 00SD, 00TA, 00TE, 00TN, 00TS, 00TX, 00UT, 00VA, 00VI, 00W, 00WA, 00WI, 00WN, 00WV, 00WY, 00XS, 01A, 01AK, 01AL, 01AR, 01AZ, 01C, 01CA, 01CL, 01CN, 01CO, 01CT, 01FA, 01FD, 01FL, 01GA, 01GE, 01IA, 01ID, 01II, 01IL, 01IN, 01IS, 01J, 01K, 01KS, 01KY, 01LA, 01LL, 01LS, 01MA, 01MD, 01ME, 01MI, 01MN, 01MO, 01MT, 01NC, 01NE, 01NH, 01NJ, 01NM, 01NV, 01NY, 01OI, 01OK, 01OR, 01PA, 01PN, 01PS, 01SC, 01TA, 01TE, 01TN, 01TS, 01TX, 01U, 01UT, 01VA, 01WA, 01WI, 01WN, 01WT, 01WY, 01XA, 01XS, 02AK, 02..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idents: Array[String] = airports.select('ident).limit(200).distinct.as[String].collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "litArray = Array(00A, 00AA, 00AK, 00AL, 00AR, 00AS, 00AZ, 00CA, 00CL, 00CN, 00CO, 00FA, 00FD, 00FL, 00GA, 00GE, 00HI, 00ID, 00IG, 00II, 00IL, 00IN, 00IS, 00KS, 00KY, 00LA, 00LL, 00LS, 00MD, 00MI, 00MN, 00MO, 00MT, 00N, 00NC, 00NJ, 00NK, 00NY, 00OH, 00OI, 00OK, 00OR, 00PA, 00PN, 00PS, 00S, 00SC, 00SD, 00TA, 00TE, 00TN, 00TS, 00TX, 00UT, 00VA, 00VI, 00W, 00WA, 00WI, 00WN, 00WV, 00WY, 00XS, 01A, 01AK, 01AL, 01AR, 01AZ, 01C, 01CA, 01CL, 01CN, 01CO, 01CT, 01FA, 01FD, 01FL, 01GA, 01GE, 01IA, 01ID, 01II, 01IL, 01IN, 01IS, 01J, 01K, 01KS, 01KY, 01LA, 01LL, 01LS, 01MA, 01MD, 01ME, 01MI, 01MN, 01MO, 01MT, 01NC, 01NE, 01NH, 01NJ, 01NM, 01NV, 01NY, 01OI, 01OK, 01OR, 01PA, 01PN, 01PS, 01SC, 01TA, 01TE, 01TN, 01TS, 01TX, 01U,...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(00A, 00AA, 00AK, 00AL, 00AR, 00AS, 00AZ, 00CA, 00CL, 00CN, 00CO, 00FA, 00FD, 00FL, 00GA, 00GE, 00HI, 00ID, 00IG, 00II, 00IL, 00IN, 00IS, 00KS, 00KY, 00LA, 00LL, 00LS, 00MD, 00MI, 00MN, 00MO, 00MT, 00N, 00NC, 00NJ, 00NK, 00NY, 00OH, 00OI, 00OK, 00OR, 00PA, 00PN, 00PS, 00S, 00SC, 00SD, 00TA, 00TE, 00TN, 00TS, 00TX, 00UT, 00VA, 00VI, 00W, 00WA, 00WI, 00WN, 00WV, 00WY, 00XS, 01A, 01AK, 01AL, 01AR, 01AZ, 01C, 01CA, 01CL, 01CN, 01CO, 01CT, 01FA, 01FD, 01FL, 01GA, 01GE, 01IA, 01ID, 01II, 01IL, 01IN, 01IS, 01J, 01K, 01KS, 01KY, 01LA, 01LL, 01LS, 01MA, 01MD, 01ME, 01MI, 01MN, 01MO, 01MT, 01NC, 01NE, 01NH, 01NJ, 01NM, 01NV, 01NY, 01OI, 01OK, 01OR, 01PA, 01PN, 01PS, 01SC, 01TA, 01TE, 01TN, 01TS, 01TX, 01U,..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val litArray = idents.map(x => lit(x))\n",
    "\n",
    "val someArray = array(litArray:_*)\n",
    "\n",
    "val result = shuffle(someArray)(0)\n",
    "\n",
    "// val identSdf = sdf.withColumn(\"ident\", shuffle(array(idents.map(lit(_)):_*))(0))\n",
    "val identSdf = sdf.withColumn(\"ident\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "identPqSink = org.apache.spark.sql.streaming.DataStreamWriter@4fc975c2\n",
       "identPqSq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@61525207\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@61525207"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val identPqSink = createParquetSink(identSdf, \"s2.parquet\")\n",
    "val identPqSq = identPqSink.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что данные записываются в `parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346687\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-07 20:36:21.019|269  |03FA |\n",
      "|2020-06-07 20:36:37.019|285  |01WY |\n",
      "|2020-06-07 20:36:53.019|301  |00IS |\n",
      "|2020-06-07 20:37:09.019|317  |02ID |\n",
      "|2020-06-07 20:37:25.019|333  |02AZ |\n",
      "+-----------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "identPq = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val identPq = spark.read.parquet(\"datasets/s2.parquet\")\n",
    "println(identPq.count)\n",
    "identPq.printSchema\n",
    "identPq.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Временно остановим стрим, он понадобится нам для следующих экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- `rate` - самый простой способ создать стрим для тестирования приложений\n",
    "- стрим начинает работу после вызова метода `start` и не блокирует основной поток программы\n",
    "- в одном Spark приложении может работать несколько стримов одновременно\n",
    "\n",
    "## File Streaming\n",
    "Spark позволяет запустить стрим, который будет \"слушать\" директорию и читать из нее новые файлы. При этом за раз будет прочитано количество файлов, установленное в параметре `maxFilesPerTrigger` [ссылка](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources). В этом кроется одна из основных проблем данного источника. Поскольку стрим, сконфигурированный под чтение небольших файлов, может \"упасть\", если в директорию начнут попадать файлы большого объема. Создадим стрим из директории `datasets/s2.parquet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalArgumentException\n",
       "Message: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:233)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:95)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:95)\n",
       "  at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:33)\n",
       "  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:215)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdfFromParquet = spark\n",
    "        .readStream\n",
    "        .format(\"parquet\")\n",
    "        .option(\"maxFilesPerTrigger\", \"1\")\n",
    "        .option(\"path\", \"datasets/s2.parquet\")\n",
    "        .load\n",
    "\n",
    "sdfFromParquet.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"total 1200\n",
       "drwxr-xr-x  153 t3nq  staff   4.8K Jun 15 20:13 .\n",
       "drwxr-xr-x   18 t3nq  staff   576B Jun 15 20:12 ..\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun 15 20:13 .part-00000-3f4ce802-057c-4841-a619-ebdb94d67e8a-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun 15 20:13 .part-00000-4a690777-b07a-4e9d-b032-083ac29282f9-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun 15 20:13 .part-00000-5a69533e-65bb-4599-b44d-99d8e205f9fe-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun 15 20:13 .part-00000-9c287510-1f37-4abd-a5ee-dc4a48f4b1dc-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun 15 20:13 .part-00000-a9edd022-ef9a-4c6d-863b-833ab82a0dca-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun 15 20:12 .part-00000-bf...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"ls -alh datasets/s2.parquet\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку в директорию могут попасть любые данные, а df должен иметь фиксированную схему, то Spark не позволяет нам создавать SDF на основе файлов без указания схемы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "letters = List(a, b, c, d, e, f, g, i)\n",
       "condition = (((((((startswith(ident, a) OR startswith(ident, b)) OR startswith(ident, c)) OR startswith(ident, d)) OR startswith(ident, e)) OR startswith(ident, f)) OR startswith(ident, g)) OR startswith(ident, i))\n",
       "sdfFromParquet = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val letters = List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"i\")\n",
    "val condition = letters.map { x => col(\"ident\").startsWith(x) }.reduce { (x,y) => x or y }\n",
    "\n",
    "val sdfFromParquet = spark\n",
    "        .readStream\n",
    "        .format(\"parquet\")\n",
    "        .schema(identPq.schema)\n",
    "        .option(\"maxFilesPerTrigger\", \"10\")\n",
    "        .option(\"path\", \"datasets/s2.parquet\")\n",
    "        .load\n",
    "        .withColumn(\"ident\", lower('ident))\n",
    "\n",
    "sdfFromParquet.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-15 20:12:35.407|0    |02ps |\n",
      "|2020-06-15 20:12:36.407|1    |00ts |\n",
      "|2020-06-15 20:12:37.407|2    |00wa |\n",
      "|2020-06-15 20:12:38.407|3    |00il |\n",
      "|2020-06-15 20:12:40.407|5    |02ne |\n",
      "|2020-06-15 20:12:41.407|6    |01ks |\n",
      "|2020-06-15 20:12:42.407|7    |02ge |\n",
      "|2020-06-15 20:12:43.407|8    |00or |\n",
      "|2020-06-15 20:12:39.407|4    |01k  |\n",
      "+-----------------------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "consoleSink = org.apache.spark.sql.streaming.DataStreamWriter@7b850df9\n",
       "sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@2e750a7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@2e750a7"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-15 20:12:44.407|9    |01nv |\n",
      "|2020-06-15 20:12:45.407|10   |00nj |\n",
      "|2020-06-15 20:12:46.407|11   |00wy |\n",
      "|2020-06-15 20:12:47.407|12   |00ak |\n",
      "|2020-06-15 20:12:48.407|13   |02wi |\n",
      "|2020-06-15 20:12:49.407|14   |00sd |\n",
      "|2020-06-15 20:12:50.407|15   |01ls |\n",
      "|2020-06-15 20:12:51.407|16   |02ny |\n",
      "|2020-06-15 20:12:52.407|17   |02or |\n",
      "|2020-06-15 20:12:53.407|18   |00cn |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-15 20:12:54.407|19   |03ak |\n",
      "|2020-06-15 20:12:55.407|20   |02ia |\n",
      "|2020-06-15 20:12:56.407|21   |00or |\n",
      "|2020-06-15 20:12:57.407|22   |02nj |\n",
      "|2020-06-15 20:12:58.407|23   |02tn |\n",
      "|2020-06-15 20:12:59.407|24   |00mi |\n",
      "|2020-06-15 20:13:00.407|25   |01fl |\n",
      "|2020-06-15 20:13:01.407|26   |03fa |\n",
      "|2020-06-15 20:13:02.407|27   |02nv |\n",
      "|2020-06-15 20:13:03.407|28   |02cl |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-15 20:13:04.407|29   |02pr |\n",
      "|2020-06-15 20:13:05.407|30   |01ge |\n",
      "|2020-06-15 20:13:06.407|31   |00wn |\n",
      "|2020-06-15 20:13:07.407|32   |01va |\n",
      "|2020-06-15 20:13:08.407|33   |01il |\n",
      "|2020-06-15 20:13:10.407|35   |00ps |\n",
      "|2020-06-15 20:13:13.407|38   |00oh |\n",
      "|2020-06-15 20:13:09.407|34   |02p  |\n",
      "|2020-06-15 20:13:11.407|36   |01a  |\n",
      "|2020-06-15 20:13:12.407|37   |00n  |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-15 20:13:14.407|39   |00la |\n",
      "|2020-06-15 20:13:15.407|40   |00md |\n",
      "|2020-06-15 20:13:16.407|41   |00ts |\n",
      "|2020-06-15 20:13:17.407|42   |02al |\n",
      "|2020-06-15 20:13:18.407|43   |02me |\n",
      "|2020-06-15 20:13:19.407|44   |00mn |\n",
      "|2020-06-15 20:13:21.407|46   |01sc |\n",
      "|2020-06-15 20:13:22.407|47   |02fl |\n",
      "|2020-06-15 20:13:23.407|48   |02mi |\n",
      "|2020-06-15 20:13:20.407|45   |01u  |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-15 20:13:24.407|49   |02nc |\n",
      "|2020-06-15 20:13:25.407|50   |01pn |\n",
      "|2020-06-15 20:13:26.407|51   |02te |\n",
      "|2020-06-15 20:13:28.407|53   |02nj |\n",
      "|2020-06-15 20:13:29.407|54   |01al |\n",
      "|2020-06-15 20:13:30.407|55   |00aa |\n",
      "|2020-06-15 20:13:31.407|56   |02hi |\n",
      "|2020-06-15 20:13:33.407|58   |03az |\n",
      "|2020-06-15 20:13:27.407|52   |00w  |\n",
      "|2020-06-15 20:13:32.407|57   |00w  |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-15 20:13:34.407|59   |00il |\n",
      "|2020-06-15 20:13:35.407|60   |01pa |\n",
      "|2020-06-15 20:13:36.407|61   |00wi |\n",
      "|2020-06-15 20:13:37.407|62   |00wn |\n",
      "|2020-06-15 20:13:38.407|63   |02ky |\n",
      "|2020-06-15 20:13:39.407|64   |00aa |\n",
      "|2020-06-15 20:13:40.407|65   |00ny |\n",
      "|2020-06-15 20:13:41.407|66   |00fd |\n",
      "|2020-06-15 20:13:42.407|67   |01wa |\n",
      "|2020-06-15 20:13:43.407|68   |02va |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-15 20:13:44.407|69   |02oi |\n",
      "|2020-06-15 20:13:45.407|70   |00pn |\n",
      "|2020-06-15 20:13:46.407|71   |01wi |\n",
      "|2020-06-15 20:13:47.407|72   |01ll |\n",
      "|2020-06-15 20:13:48.407|73   |01wa |\n",
      "|2020-06-15 20:13:49.407|74   |01xa |\n",
      "|2020-06-15 20:13:50.407|75   |02md |\n",
      "|2020-06-15 20:13:51.407|76   |00te |\n",
      "|2020-06-15 20:13:52.407|77   |00hi |\n",
      "|2020-06-15 20:13:53.407|78   |03aa |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-15 20:13:55.407|80   |01cl |\n",
      "|2020-06-15 20:13:56.407|81   |02ny |\n",
      "|2020-06-15 20:13:57.407|82   |00wv |\n",
      "|2020-06-15 20:13:58.407|83   |00ge |\n",
      "|2020-06-15 20:13:59.407|84   |00fl |\n",
      "|2020-06-15 20:14:00.407|85   |00ii |\n",
      "|2020-06-15 20:14:01.407|86   |00xs |\n",
      "|2020-06-15 20:14:02.407|87   |01ma |\n",
      "|2020-06-15 20:13:54.407|79   |00w  |\n",
      "|2020-06-15 20:14:03.407|88   |01k  |\n",
      "+-----------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val consoleSink = createConsoleSink(sdfFromParquet)\n",
    "val sq = consoleSink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped FileStreamSource[file:/Users/t3nq/Projects/smz/de-spark-scala/datasets/s2.parquet]\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File source позволяет со всеми типами файлов, с которыми умеет работать Spark: `parquet`, `orc`, `csv`, `json`, `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(sq.status.isTriggerActive) {\n",
    "    Unit\n",
    "}\n",
    "\n",
    "sq.stop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Spark позволяет создавать SDF на базе всех поддерживаемых типов файлов\n",
    "- При создании SDF вы должны указать схему данных\n",
    "- File streaming имеет несколько серьезных недостатков:\n",
    "  + Входной поток можно ограничить только макисмальным количество файлов, попадающих в батч\n",
    "  + Если стрим упадает посередине файла, то при перезапуске эти данные будут обработаны еще раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"100\" height=\"100\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Apache_kafka.svg/1200px-Apache_kafka.svg.png\">\n",
    "\n",
    "## Kafka streaming\n",
    "\n",
    "https://kafka.apache.org\n",
    "\n",
    "**Apache Kafka** - самая распространенная в мире система, на основе которой строятся приложения для поточной обработки данных. Она имеет несколько преимуществ:\n",
    "- высокая пропускная способность\n",
    "- высокая доступность за счет распределенной архитектуры и репликации\n",
    "- у каждого сообщения есть свой номер, который называется offset, что позволяет гранулярно сохранять состояние стрима\n",
    "\n",
    "### Архитектура системы\n",
    "\n",
    "#### Topic\n",
    "Топик - это таблицы в Kafka. Мы пишем данные в топик и читаем данные из топика. Топик как правило распределен по нескольким узлам кластера для обеспечения высокой доступности и скорости работы с данными\n",
    "\n",
    "<img align=\"center\" width=\"500\" height=\"500\" src=\"https://kafka.apache.org/25/images/log_anatomy.png\">\n",
    "\n",
    "#### Partition\n",
    "Партиции - это блоки, из которых состоят топики. Партиция представляет собой неделимый блок, который хранится на одном из узлов. Топик может иметь произвольное количество партиций. Чем больше партиций - тем выше параллелзим при чтении и записи, однако слишком большое число партиций в топике может привести к замедлению работы всей системы.\n",
    "\n",
    "#### Replica\n",
    "Каждая партиция имеет (может иметь) несколько реплик. Внешние приложения всегда работают (читают и пишут) с основной репликой. Остальные реплики являются дочерними и не используются во внешнем IO. Если узел, на котором расположена основная реплика, падает, то одна из дочерних реплик становится основной и работа с данными продолжается\n",
    "\n",
    "#### Message\n",
    "Сообщения - это данные, которые мы пишем и читаем в Kafka. Они представлены кортежем (Key, Value), но ключ может быть иметь значение `null` (используется не всегда). Сереализация и десереализация данных всегда происходит на уровне клиентов Kafka. Сама Kafka ничего о типах данных не знает и хранит ключи и значения в виде массива байт\n",
    "\n",
    "#### Offset\n",
    "Оффсет - это порядковый номер сообщения в партиции. Когда мы пишем сообщение (сообщение всегда пишется в одну из партиций топика), Kafka помещает его в топик с номер `n+1`, где `n` - номер последнего сообщения в этом топике\n",
    "\n",
    "<img align=\"center\" width=\"400\" height=\"400\" src=\"https://kafka.apache.org/25/images/log_consumer.png\">\n",
    "\n",
    "#### Producer\n",
    "Producer - это приложение, которое пишет в топик. Producer'ов может быть много. Параллельная запись достигается за счет того, что каждое новое сообщение попадает в случайную партицию топика (если не указан `key`)\n",
    "\n",
    "#### Consumer\n",
    "Consumer - это приложение, читающее данные из топика. Consumer'ов может быть много, в этом случае они называются `consumer group`. Параллельное чтение достигается за счет распределения партиций топика между consumer'ами в рамках одной группы. Каждый consumer читает данные из \"своих\" партиций и ничего про другие не знает. Если consumer падает, то \"его\" партиции переходят другим consumer'ам.\n",
    "\n",
    "#### Commit\n",
    "Коммитом в Kafka называют сохранение информации о факте обработки сообщения с определенным оффсетом. Поскольку оффсеты для каждой партиции топика свои, то и информация о последнем обработанном оффсете хранится по каждой партиции отдельно. Обычные приложения пишут коммиты в специальный топик Kafka, который имеет название `__consumer_offsets`. Spark хранит обработанные оффсеты по каждому батчу в ФС (например, в HDFS).\n",
    "\n",
    "#### Retention\n",
    "Поскольку кластер Kafka не может хранить данные вечно, то в ее конфигурации задаются пороговые значение по **объему** и **времени хранения** для каждого топика, при превышении которых данные удаляются. Например, если у топика A установлен renention по времени 1 месяц, то данные будут хранится в системе не менее одного месяца (и затем будут удалены одной из внутренних подсистем)\n",
    "\n",
    "### Spark connector\n",
    "https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10  \n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html  \n",
    "\n",
    "### Запуск Kafka в docker\n",
    "```shell\n",
    "docker run --rm \\\n",
    "   -p 2181:2181 \\\n",
    "   --name=test_zoo \\\n",
    "   -e ZOOKEEPER_CLIENT_PORT=2181 \\\n",
    "   confluentinc/cp-zookeeper\n",
    "```\n",
    "\n",
    "```shell\n",
    "docker run --rm \\\n",
    "    -p 9092:9092 \\\n",
    "    --name=test_kafka \\\n",
    "    -e KAFKA_ZOOKEEPER_CONNECT=host.docker.internal:2181 \\\n",
    "    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\n",
    "    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\n",
    "    confluentinc/cp-kafka\n",
    "```\n",
    "\n",
    "### Работа с Kafka с помощь Static Dataframe\n",
    "\n",
    "Spark позволяет работать с кафкой как с обычной базой данных. Запишем данные в топик `test_topic0`. Для этого нам необходимо подготовить DF, в котором будет две колонки:\n",
    "- `value: String` - данные, которые мы хотим записать\n",
    "- `topic: String` - топик, куда писать каждую строку DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-15 20:12:35.407|0    |02PS |\n",
      "|2020-06-15 20:12:36.407|1    |00TS |\n",
      "|2020-06-15 20:12:37.407|2    |00WA |\n",
      "|2020-06-15 20:12:38.407|3    |00IL |\n",
      "|2020-06-15 20:12:40.407|5    |02NE |\n",
      "+-----------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "identPq = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val identPq = spark.read.parquet(\"datasets/s2.parquet\").limit(200)\n",
    "println(identPq.count)\n",
    "identPq.printSchema\n",
    "identPq.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "writeKafka: [T](topic: String, data: org.apache.spark.sql.Dataset[T])Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def writeKafka[T](topic: String, data: Dataset[T]): Unit = {\n",
    "    val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"localhost:9092\"\n",
    "    )\n",
    "    \n",
    "    data.toJSON.withColumn(\"topic\", lit(topic)).write.format(\"kafka\").options(kafkaParams).save\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeKafka(\"test_topic0\", identPq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем данные из Kafka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     0|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     1|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     2|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     3|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     4|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     5|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     6|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     7|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     8|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     9|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    10|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    11|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    12|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    13|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    14|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    15|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    16|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    17|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    18|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    19|2020-06-15 21:15:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------------------+---------+-----+\n",
      "|topic                      |partition|count|\n",
      "+---------------------------+---------+-----+\n",
      "|test_topic0                |0        |94   |\n",
      "|__confluent.support.metrics|0        |1    |\n",
      "+---------------------------+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> localhost:9092, subscribePattern -> .*)\n",
       "df = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"localhost:9092\",\n",
    "        \"subscribePattern\" -> \".*\"\n",
    "    )\n",
    "\n",
    "\n",
    "val df = spark.read.format(\"kafka\").options(kafkaParams).load\n",
    "\n",
    "df.printSchema\n",
    "df.show\n",
    "\n",
    "df.groupBy('topic, 'partition).count.show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение из Kafka имеет несколько особенностей:\n",
    "- по умолчанию читается все содержимое топика. Поскольку обычно в нем много данных, эта операция может создать большую нагрузку на кластер Kafka и Spark приложение\n",
    "- колонки `value` и `key` имеют тип `binary`, который необходимо десереализовать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы прочитать только определенную часть топика, нам необходимо задать минимальный и максимальный оффсет для чтения с помощью параметров `startingOffsets` , `endingOffsets`. Возьмем два случайных события:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------+\n",
      "|      topic|partition|offset|\n",
      "+-----------+---------+------+\n",
      "|test_topic0|        0|     9|\n",
      "|test_topic0|        0|    11|\n",
      "+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(0.1).limit(2).select('topic, 'partition, 'offset).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основании этих событий подготовим параметры `startingOffsets` и `endingOffsets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     9|2020-06-15 21:15:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    10|2020-06-15 21:15:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> test_topic0, startingOffsets -> \" { \"test_topic0\": { \"0\": 9 } } \", endingOffsets -> \" { \"test_topic0\": { \"0\": 11 } }  \")\n",
       "df = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"localhost:9092\",\n",
    "        \"subscribe\" -> \"test_topic0\",\n",
    "        \"startingOffsets\" -> \"\"\" { \"test_topic0\": { \"0\": 9 } } \"\"\",\n",
    "        \"endingOffsets\" -> \"\"\" { \"test_topic0\": { \"0\": 11 } }  \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "val df = spark.read.format(\"kafka\").options(kafkaParams).load\n",
    "\n",
    "df.printSchema\n",
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию параметр `startingOffsets` имеет значение `earliest`, а `endingOffsets` - `latest`. Поэтому, когда мы не указывали эти параметры, Spark прочитал содержимое всего топика\n",
    "\n",
    "Чтобы получить наши данные, которые мы записали в топик, нам необходимо их десереализовать. В нашем случае достаточно использовать `.cast(\"string\")`, однако это работает не всегда, т.к. формат данных может быть произвольным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+\n",
      "|value                                                                  |\n",
      "+-----------------------------------------------------------------------+\n",
      "|{\"timestamp\":\"2020-06-15T20:12:45.407+03:00\",\"value\":10,\"ident\":\"00NJ\"}|\n",
      "|{\"timestamp\":\"2020-06-15T20:12:46.407+03:00\",\"value\":11,\"ident\":\"00WY\"}|\n",
      "+-----------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "+-----+-----------------------------+-----+\n",
      "|ident|timestamp                    |value|\n",
      "+-----+-----------------------------+-----+\n",
      "|00NJ |2020-06-15T20:12:45.407+03:00|10   |\n",
      "|00WY |2020-06-15T20:12:46.407+03:00|11   |\n",
      "+-----+-----------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jsonString = [value: string]\n",
       "parsed = [ident: string, timestamp: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, timestamp: string ... 1 more field]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonString = df.select('value.cast(\"string\")).as[String]\n",
    "\n",
    "jsonString.show(20, false)\n",
    "\n",
    "val parsed = spark.read.json(jsonString)\n",
    "parsed.printSchema\n",
    "parsed.show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с Kafka с помощью Streaming DF\n",
    "При создании SDF из Kafka необходимо помнить, что:\n",
    "- `startingOffsets` по умолчанию имеет значение `latest`\n",
    "- `endingOffsets` использовать нельзя\n",
    "- количество сообщений за батч можно (и нужно) ограничить параметром `maxOffsetPerTrigger` (по умолчанию он не задан и первый батч будет содержать данные всего топика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> test_topic0, startingOffsets -> earliest, maxOffsetsPerTrigger -> 5)\n",
       "sdf = [key: binary, value: binary ... 5 more fields]\n",
       "parsedSdf = [value: string, topic: string ... 2 more fields]\n",
       "sink = org.apache.spark.sql.streaming.DataStreamWriter@1ead7eec\n",
       "sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3f8e8a17\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3f8e8a17"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                 |topic      |partition|offset|\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-15T20:12:35.407+03:00\",\"value\":0,\"ident\":\"02PS\"}|test_topic0|0        |0     |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:36.407+03:00\",\"value\":1,\"ident\":\"00TS\"}|test_topic0|0        |1     |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:37.407+03:00\",\"value\":2,\"ident\":\"00WA\"}|test_topic0|0        |2     |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:38.407+03:00\",\"value\":3,\"ident\":\"00IL\"}|test_topic0|0        |3     |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:40.407+03:00\",\"value\":5,\"ident\":\"02NE\"}|test_topic0|0        |4     |\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                  |topic      |partition|offset|\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-15T20:12:41.407+03:00\",\"value\":6,\"ident\":\"01KS\"} |test_topic0|0        |5     |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:42.407+03:00\",\"value\":7,\"ident\":\"02GE\"} |test_topic0|0        |6     |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:43.407+03:00\",\"value\":8,\"ident\":\"00OR\"} |test_topic0|0        |7     |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:44.407+03:00\",\"value\":9,\"ident\":\"01NV\"} |test_topic0|0        |8     |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:45.407+03:00\",\"value\":10,\"ident\":\"00NJ\"}|test_topic0|0        |9     |\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                  |topic      |partition|offset|\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-15T20:12:46.407+03:00\",\"value\":11,\"ident\":\"00WY\"}|test_topic0|0        |10    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:47.407+03:00\",\"value\":12,\"ident\":\"00AK\"}|test_topic0|0        |11    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:48.407+03:00\",\"value\":13,\"ident\":\"02WI\"}|test_topic0|0        |12    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:49.407+03:00\",\"value\":14,\"ident\":\"00SD\"}|test_topic0|0        |13    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:50.407+03:00\",\"value\":15,\"ident\":\"01LS\"}|test_topic0|0        |14    |\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                  |topic      |partition|offset|\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-15T20:12:51.407+03:00\",\"value\":16,\"ident\":\"02NY\"}|test_topic0|0        |15    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:52.407+03:00\",\"value\":17,\"ident\":\"02OR\"}|test_topic0|0        |16    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:53.407+03:00\",\"value\":18,\"ident\":\"00CN\"}|test_topic0|0        |17    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:54.407+03:00\",\"value\":19,\"ident\":\"03AK\"}|test_topic0|0        |18    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:55.407+03:00\",\"value\":20,\"ident\":\"02IA\"}|test_topic0|0        |19    |\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"localhost:9092\",\n",
    "        \"subscribe\" -> \"test_topic0\",\n",
    "        \"startingOffsets\" -> \"\"\"earliest\"\"\",\n",
    "        \"maxOffsetsPerTrigger\" -> \"5\"\n",
    "    )\n",
    "\n",
    "val sdf = spark.readStream.format(\"kafka\").options(kafkaParams).load\n",
    "val parsedSdf = sdf.select('value.cast(\"string\"), 'topic, 'partition, 'offset)\n",
    "\n",
    "val sink = createConsoleSink(parsedSdf)\n",
    "\n",
    "val sq = sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[test_topic0]]\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы перезапустим этот стрим, он повторно прочитает все данные. Чтобы обеспечить сохранение состояния стрима после обработки каждого батча, нам необходимо добавить параметр `checkpointLocation` в опции `writeStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSinkWithCheckpoint: (chkName: String, df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createConsoleSinkWithCheckpoint(chkName: String, df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"checkpointLocation\", s\"chk/$chkName\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"v1\n",
       "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1592246100006,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"200\"}}\n",
       "{\"test_topic0\":{\"0\":30}}\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"cat chk/test0/offsets/5\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sink = org.apache.spark.sql.streaming.DataStreamWriter@254ca66c\n",
       "sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@333fee21\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@333fee21"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                  |topic      |partition|offset|\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-15T20:12:51.407+03:00\",\"value\":16,\"ident\":\"02NY\"}|test_topic0|0        |15    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:52.407+03:00\",\"value\":17,\"ident\":\"02OR\"}|test_topic0|0        |16    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:53.407+03:00\",\"value\":18,\"ident\":\"00CN\"}|test_topic0|0        |17    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:54.407+03:00\",\"value\":19,\"ident\":\"03AK\"}|test_topic0|0        |18    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:55.407+03:00\",\"value\":20,\"ident\":\"02IA\"}|test_topic0|0        |19    |\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                  |topic      |partition|offset|\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-15T20:12:56.407+03:00\",\"value\":21,\"ident\":\"00OR\"}|test_topic0|0        |20    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:57.407+03:00\",\"value\":22,\"ident\":\"02NJ\"}|test_topic0|0        |21    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:58.407+03:00\",\"value\":23,\"ident\":\"02TN\"}|test_topic0|0        |22    |\n",
      "|{\"timestamp\":\"2020-06-15T20:12:59.407+03:00\",\"value\":24,\"ident\":\"00MI\"}|test_topic0|0        |23    |\n",
      "|{\"timestamp\":\"2020-06-15T20:13:00.407+03:00\",\"value\":25,\"ident\":\"01FL\"}|test_topic0|0        |24    |\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                  |topic      |partition|offset|\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-15T20:13:01.407+03:00\",\"value\":26,\"ident\":\"03FA\"}|test_topic0|0        |25    |\n",
      "|{\"timestamp\":\"2020-06-15T20:13:02.407+03:00\",\"value\":27,\"ident\":\"02NV\"}|test_topic0|0        |26    |\n",
      "|{\"timestamp\":\"2020-06-15T20:13:03.407+03:00\",\"value\":28,\"ident\":\"02CL\"}|test_topic0|0        |27    |\n",
      "|{\"timestamp\":\"2020-06-15T20:13:04.407+03:00\",\"value\":29,\"ident\":\"02PR\"}|test_topic0|0        |28    |\n",
      "|{\"timestamp\":\"2020-06-15T20:13:05.407+03:00\",\"value\":30,\"ident\":\"01GE\"}|test_topic0|0        |29    |\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sink = createConsoleSinkWithCheckpoint(\"test0\", parsedSdf)\n",
    "val sq = sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6b9f5188"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n",
      "false\n"
     ]
    }
   ],
   "source": [
    "parsedSdf.writeStream.foreachBatch { (batchDf, batchId) => println(batchDf.isStreaming) }.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[test_topic0]]\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Apache Kafka - распределенная система, обеспечивающая передачу потока данных в слабосвязанных системах\n",
    "- Работать с Kafka можно как с использованием Static DF, так и с помощью Streaming DF\n",
    "- Чтобы стрим запоминал свое состояние после остановки, необходимо использовать checkpoint - директорию на HDFS (или локальной ФС), в которую будет сохранятся состояние стрима после каждого батча"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце работы не забудьте остановить Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "localVec = Vector({\"timestamp\":\"2020-06-15T20:12:56.407+03:00\",\"value\":21,\"ident\":\"00OR\"})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector({\"timestamp\":\"2020-06-15T20:12:56.407+03:00\",\"value\":21,\"ident\":\"00OR\"})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val localVec = Vector(\"\"\"{\"timestamp\":\"2020-06-15T20:12:56.407+03:00\",\"value\":21,\"ident\":\"00OR\"}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = localVec.toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- foo[0].bar: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "docType = StructType(StructField(value,StringType,true), StructField(timestamp,TimestampType,true), StructField(ident,StringType,true), StructField(foo,ArrayType(StructType(StructField(bar,StringType,true)),true),true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(value,StringType,true), StructField(timestamp,TimestampType,true), StructField(ident,StringType,true), StructField(foo,ArrayType(StructType(StructField(bar,StringType,true)),true),true))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get_json_object\n",
    "// json_tuple\n",
    "// from_json\n",
    "import org.apache.spark.sql.functions._\n",
    "// df.select(get_json_object('value, \"$.value\").alias(\"foo\")).show\n",
    "// df.select(json_tuple('value, \"timestamp\", \"value\", \"ident\").as(Array(\"timestamp\", \"value\", \"ident\"))).show\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val docType = StructType(\n",
    "    StructField(\"value\", StringType) ::\n",
    "    StructField(\"timestamp\", TimestampType) ::\n",
    "    StructField(\"ident\", StringType) :: \n",
    "    StructField(\"foo\", ArrayType(StructType(\n",
    "        StructField(\"bar\", StringType) :: Nil\n",
    "    ))) :: Nil\n",
    ")\n",
    "\n",
    "df.select(from_json('value, docType).alias(\"foo\")).select(col(\"foo.*\")).select(col(\"foo\")(0)(\"bar\")).printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

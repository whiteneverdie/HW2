{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Structured Streaming I\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Общие сведения\n",
    "+ Rate streaming\n",
    "+ File streaming\n",
    "+ Kafka streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общие сведения\n",
    "\n",
    "Системы поточной обработки данных:\n",
    "- работают с непрерывным потоком данных\n",
    "- нужно хранить состояние стрима\n",
    "- результат обработки быстро появляется в целевой системе\n",
    "- должны проектироваться с учетом требований к высокой доступности\n",
    "- важная скорость обработки данных и время зажержки (лаг)\n",
    "\n",
    "### Примеры систем поточной обработки данных\n",
    "\n",
    "#### Карточный процессинг\n",
    "- нельзя терять платежи\n",
    "- нельзя дублировать платежи\n",
    "- простой сервиса недопустим\n",
    "- максимальное время задержки ~ 1 сек\n",
    "- небольшой поток событий\n",
    "- OLTP\n",
    "\n",
    "#### Обработка логов безопасности\n",
    "- потеря единичных событий допустима\n",
    "- дублирование единичных событий допустимо\n",
    "- простой сервиса допустим\n",
    "- максимальное время задержки ~ 1 час\n",
    "- большой поток событий\n",
    "- OLAP\n",
    "\n",
    "### Виды стриминг систем\n",
    "\n",
    "#### Real-time streaming\n",
    "- низкие задержки на обработку\n",
    "- низкая пропускная способность\n",
    "- подходят для критичных систем\n",
    "- пособытийная обработка\n",
    "- OLTP\n",
    "- exactly once consistency (нет потери данных и нет дубликатов)\n",
    "\n",
    "#### Micro batch streaming\n",
    "- высокие задержки\n",
    "- высокая пропускная способность\n",
    "- не подходят для критичных систем\n",
    "- обработка батчами\n",
    "- OLAP\n",
    "- at least once consistency (во время сбоев могут возникать дубликаты)\n",
    "\n",
    "### Выводы:\n",
    "+ Существуют два типа систем поточной обработки данных - real-time и micro-batch\n",
    "+ Spark Structured Streaming является micro-batch системой\n",
    "+ При работе с большими данными обычно пропускная способность важнее, чем время задержки\n",
    "\n",
    "\n",
    "## Rate streaming\n",
    "\n",
    "Самый простой способ создать стрим - использовать `rate` источник. Созданный DF является streaming, о чем нам говорит метод создания `readStream` и атрибут `isStreaming`. `rate` хорошо подходит для тестирования приложений, когда нет возможности подключится к потоку реальных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sdf = [timestamp: timestamp, value: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf = spark.readStream.format(\"rate\").load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У `sdf`, как и у любого DF, есть схема и план выполнения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@1a90d227, rate, [timestamp#4, value#5L]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "timestamp: timestamp, value: bigint\n",
      "StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@1a90d227, rate, [timestamp#4, value#5L]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@1a90d227, rate, [timestamp#4, value#5L]\n",
      "\n",
      "== Physical Plan ==\n",
      "StreamingRelation rate, [timestamp#4, value#5L]\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema\n",
    "sdf.explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличии от обычных DF, у `sdf` нет таких методов, как `show`, `collect`, `take`. Для них также недоступен Dataset API. Поэтому для того, чтобы посмотреть их содержимое, мы должны использовать `console` синк и создать `StreamingQuery`. Процессинг начинается только после вызова метода `start`. `trigger` позволяет настроить, как часто стрим будет читать новые данные и обрабатывать их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSink: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createConsoleSink(df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sink = createConsoleSink(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@53560ece\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@53560ece"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:40:41.723|0    |\n",
      "|2020-06-07 15:40:42.723|1    |\n",
      "|2020-06-07 15:40:43.723|2    |\n",
      "|2020-06-07 15:40:44.723|3    |\n",
      "|2020-06-07 15:40:45.723|4    |\n",
      "|2020-06-07 15:40:46.723|5    |\n",
      "|2020-06-07 15:40:47.723|6    |\n",
      "|2020-06-07 15:40:48.723|7    |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:40:49.723|8    |\n",
      "|2020-06-07 15:40:50.723|9    |\n",
      "|2020-06-07 15:40:51.723|10   |\n",
      "|2020-06-07 15:40:52.723|11   |\n",
      "|2020-06-07 15:40:53.723|12   |\n",
      "|2020-06-07 15:40:54.723|13   |\n",
      "|2020-06-07 15:40:55.723|14   |\n",
      "|2020-06-07 15:40:56.723|15   |\n",
      "|2020-06-07 15:40:57.723|16   |\n",
      "|2020-06-07 15:40:58.723|17   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:40:59.723|18   |\n",
      "|2020-06-07 15:41:00.723|19   |\n",
      "|2020-06-07 15:41:01.723|20   |\n",
      "|2020-06-07 15:41:02.723|21   |\n",
      "|2020-06-07 15:41:03.723|22   |\n",
      "|2020-06-07 15:41:04.723|23   |\n",
      "|2020-06-07 15:41:05.723|24   |\n",
      "|2020-06-07 15:41:06.723|25   |\n",
      "|2020-06-07 15:41:07.723|26   |\n",
      "|2020-06-07 15:41:08.723|27   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:41:09.723|28   |\n",
      "|2020-06-07 15:41:10.723|29   |\n",
      "|2020-06-07 15:41:11.723|30   |\n",
      "|2020-06-07 15:41:12.723|31   |\n",
      "|2020-06-07 15:41:13.723|32   |\n",
      "|2020-06-07 15:41:14.723|33   |\n",
      "|2020-06-07 15:41:15.723|34   |\n",
      "|2020-06-07 15:41:16.723|35   |\n",
      "|2020-06-07 15:41:17.723|36   |\n",
      "|2020-06-07 15:41:18.723|37   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:41:19.723|38   |\n",
      "|2020-06-07 15:41:20.723|39   |\n",
      "|2020-06-07 15:41:21.723|40   |\n",
      "|2020-06-07 15:41:22.723|41   |\n",
      "|2020-06-07 15:41:23.723|42   |\n",
      "|2020-06-07 15:41:24.723|43   |\n",
      "|2020-06-07 15:41:25.723|44   |\n",
      "|2020-06-07 15:41:26.723|45   |\n",
      "|2020-06-07 15:41:27.723|46   |\n",
      "|2020-06-07 15:41:28.723|47   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:41:29.723|48   |\n",
      "|2020-06-07 15:41:30.723|49   |\n",
      "|2020-06-07 15:41:31.723|50   |\n",
      "|2020-06-07 15:41:32.723|51   |\n",
      "|2020-06-07 15:41:33.723|52   |\n",
      "|2020-06-07 15:41:34.723|53   |\n",
      "|2020-06-07 15:41:35.723|54   |\n",
      "|2020-06-07 15:41:36.723|55   |\n",
      "|2020-06-07 15:41:37.723|56   |\n",
      "|2020-06-07 15:41:38.723|57   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:41:39.723|58   |\n",
      "|2020-06-07 15:41:40.723|59   |\n",
      "|2020-06-07 15:41:41.723|60   |\n",
      "|2020-06-07 15:41:42.723|61   |\n",
      "|2020-06-07 15:41:43.723|62   |\n",
      "|2020-06-07 15:41:44.723|63   |\n",
      "|2020-06-07 15:41:45.723|64   |\n",
      "|2020-06-07 15:41:46.723|65   |\n",
      "|2020-06-07 15:41:47.723|66   |\n",
      "|2020-06-07 15:41:48.723|67   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:41:49.723|68   |\n",
      "|2020-06-07 15:41:50.723|69   |\n",
      "|2020-06-07 15:41:51.723|70   |\n",
      "|2020-06-07 15:41:52.723|71   |\n",
      "|2020-06-07 15:41:53.723|72   |\n",
      "|2020-06-07 15:41:54.723|73   |\n",
      "|2020-06-07 15:41:55.723|74   |\n",
      "|2020-06-07 15:41:56.723|75   |\n",
      "|2020-06-07 15:41:57.723|76   |\n",
      "|2020-06-07 15:41:58.723|77   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:41:59.723|78   |\n",
      "|2020-06-07 15:42:00.723|79   |\n",
      "|2020-06-07 15:42:01.723|80   |\n",
      "|2020-06-07 15:42:02.723|81   |\n",
      "|2020-06-07 15:42:03.723|82   |\n",
      "|2020-06-07 15:42:04.723|83   |\n",
      "|2020-06-07 15:42:05.723|84   |\n",
      "|2020-06-07 15:42:06.723|85   |\n",
      "|2020-06-07 15:42:07.723|86   |\n",
      "|2020-06-07 15:42:08.723|87   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:42:09.723|88   |\n",
      "|2020-06-07 15:42:10.723|89   |\n",
      "|2020-06-07 15:42:11.723|90   |\n",
      "|2020-06-07 15:42:12.723|91   |\n",
      "|2020-06-07 15:42:13.723|92   |\n",
      "|2020-06-07 15:42:14.723|93   |\n",
      "|2020-06-07 15:42:15.723|94   |\n",
      "|2020-06-07 15:42:16.723|95   |\n",
      "|2020-06-07 15:42:17.723|96   |\n",
      "|2020-06-07 15:42:18.723|97   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:42:19.723|98   |\n",
      "|2020-06-07 15:42:20.723|99   |\n",
      "|2020-06-07 15:42:21.723|100  |\n",
      "|2020-06-07 15:42:22.723|101  |\n",
      "|2020-06-07 15:42:23.723|102  |\n",
      "|2020-06-07 15:42:24.723|103  |\n",
      "|2020-06-07 15:42:25.723|104  |\n",
      "|2020-06-07 15:42:26.723|105  |\n",
      "|2020-06-07 15:42:27.723|106  |\n",
      "|2020-06-07 15:42:28.723|107  |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:42:29.723|108  |\n",
      "|2020-06-07 15:42:30.723|109  |\n",
      "|2020-06-07 15:42:31.723|110  |\n",
      "|2020-06-07 15:42:32.723|111  |\n",
      "|2020-06-07 15:42:33.723|112  |\n",
      "|2020-06-07 15:42:34.723|113  |\n",
      "|2020-06-07 15:42:35.723|114  |\n",
      "|2020-06-07 15:42:36.723|115  |\n",
      "|2020-06-07 15:42:37.723|116  |\n",
      "|2020-06-07 15:42:38.723|117  |\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sq = sink.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы остановить DF, можно вызвать метод `stop` к `sdf`, либо получить список всех streming DF и остановить их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "killAll: ()Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "def killAll() = {\n",
    "    SparkSession\n",
    "        .active\n",
    "        .streams\n",
    "        .active\n",
    "        .foreach { x =>\n",
    "                    val desc = x.lastProgress.sources.head.description\n",
    "                    x.stop\n",
    "                    println(s\"Stopped ${desc}\")\n",
    "        }               \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[test_topic0]]\n"
     ]
    }
   ],
   "source": [
    "killAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим стрим, выполняющий запись в `parquet` файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "createParquetSink: (df: org.apache.spark.sql.DataFrame, fileName: String)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createParquetSink(df: DataFrame, \n",
    "                      fileName: String) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", s\"datasets/$fileName\")\n",
    "    .option(\"checkpointLocation\", s\"chk/$fileName\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sink = org.apache.spark.sql.streaming.DataStreamWriter@343c5ff0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.streaming.DataStreamWriter@343c5ff0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sink = createParquetSink(sdf, \"s1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@188efa2d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@188efa2d"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sq = sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Убедимся, что стрим пишется в файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"total 1520\n",
       "drwxr-xr-x  193 t3nq  staff   6.0K Jun  7 15:56 .\n",
       "drwxr-xr-x   17 t3nq  staff   544B Jun  7 15:52 ..\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun  7 15:56 .part-00000-0586f3cc-6308-4ea8-b8b0-17b738a477bc-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff    12B Jun  7 15:54 .part-00000-07eb2750-a44b-470d-83c8-0f2933e42402-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun  7 15:56 .part-00000-499b60b1-359e-4085-99af-93b0dd20d702-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun  7 15:55 .part-00000-58c09c80-2f1d-4f04-8970-2673401838f6-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun  7 15:55 .part-00000-5a84c643-5909-4598-ac0f-7a5f047d10aa-c000.snappy.parquet.crc\n",
       "-rw-r--r--    1 t3nq  staff    16B Jun  7 ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\"ls -alh datasets/s1.parquet\".!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем файл с помощью Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2020-06-07 15:54:55.049|0    |\n",
      "|2020-06-07 15:54:56.049|1    |\n",
      "|2020-06-07 15:54:57.049|2    |\n",
      "|2020-06-07 15:54:58.049|3    |\n",
      "|2020-06-07 15:54:59.049|4    |\n",
      "+-----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rates = [timestamp: timestamp, value: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rates = spark.read.parquet(\"datasets/s1.parquet\")\n",
    "println(rates.count)\n",
    "rates.printSchema\n",
    "rates.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параллельно внутри одного Spark приложения может работать несколько стримов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "consoleSink = org.apache.spark.sql.streaming.DataStreamWriter@499700e8\n",
       "consoleSq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@15ff7ea9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@15ff7ea9"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---------------------+-----+\n",
      "|timestamp            |value|\n",
      "+---------------------+-----+\n",
      "|2020-06-07 15:58:49.8|0    |\n",
      "|2020-06-07 15:58:50.8|1    |\n",
      "|2020-06-07 15:58:51.8|2    |\n",
      "|2020-06-07 15:58:52.8|3    |\n",
      "|2020-06-07 15:58:53.8|4    |\n",
      "|2020-06-07 15:58:54.8|5    |\n",
      "|2020-06-07 15:58:55.8|6    |\n",
      "|2020-06-07 15:58:56.8|7    |\n",
      "|2020-06-07 15:58:57.8|8    |\n",
      "|2020-06-07 15:58:58.8|9    |\n",
      "+---------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---------------------+-----+\n",
      "|timestamp            |value|\n",
      "+---------------------+-----+\n",
      "|2020-06-07 15:58:59.8|10   |\n",
      "|2020-06-07 15:59:00.8|11   |\n",
      "|2020-06-07 15:59:01.8|12   |\n",
      "|2020-06-07 15:59:02.8|13   |\n",
      "|2020-06-07 15:59:03.8|14   |\n",
      "|2020-06-07 15:59:04.8|15   |\n",
      "|2020-06-07 15:59:05.8|16   |\n",
      "|2020-06-07 15:59:06.8|17   |\n",
      "|2020-06-07 15:59:07.8|18   |\n",
      "|2020-06-07 15:59:08.8|19   |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val consoleSink = createConsoleSink(sdf)\n",
    "val consoleSq = consoleSink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая добавляет к нашей колонке случайный `ident` аэропорта из датасета [Airport Codes](https://datahub.io/core/airport-codes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"datasets/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idents = Array(00A, 00AA, 00AK, 00AL, 00AR, 00AS, 00AZ, 00CA, 00CL, 00CN, 00CO, 00FA, 00FD, 00FL, 00GA, 00GE, 00HI, 00ID, 00IG, 00II, 00IL, 00IN, 00IS, 00KS, 00KY, 00LA, 00LL, 00LS, 00MD, 00MI, 00MN, 00MO, 00MT, 00N, 00NC, 00NJ, 00NK, 00NY, 00OH, 00OI, 00OK, 00OR, 00PA, 00PN, 00PS, 00S, 00SC, 00SD, 00TA, 00TE, 00TN, 00TS, 00TX, 00UT, 00VA, 00VI, 00W, 00WA, 00WI, 00WN, 00WV, 00WY, 00XS, 01A, 01AK, 01AL, 01AR, 01AZ, 01C, 01CA, 01CL, 01CN, 01CO, 01CT, 01FA, 01FD, 01FL, 01GA, 01GE, 01IA, 01ID, 01II, 01IL, 01IN, 01IS, 01J, 01K, 01KS, 01KY, 01LA, 01LL, 01LS, 01MA, 01MD, 01ME, 01MI, 01MN, 01MO, 01MT, 01NC, 01NE, 01NH, 01NJ, 01NM, 01NV, 01NY, 01OI, 01OK, 01OR, 01PA, 01PN, 01PS, 01SC, 01TA, 01TE, 01TN, 01TS, 01TX, 01U, 01UT, 01VA, 01WA, 01WI, 01WN, 01WT, 01WY, 01XA, 01XS, 02AK, 02...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(00A, 00AA, 00AK, 00AL, 00AR, 00AS, 00AZ, 00CA, 00CL, 00CN, 00CO, 00FA, 00FD, 00FL, 00GA, 00GE, 00HI, 00ID, 00IG, 00II, 00IL, 00IN, 00IS, 00KS, 00KY, 00LA, 00LL, 00LS, 00MD, 00MI, 00MN, 00MO, 00MT, 00N, 00NC, 00NJ, 00NK, 00NY, 00OH, 00OI, 00OK, 00OR, 00PA, 00PN, 00PS, 00S, 00SC, 00SD, 00TA, 00TE, 00TN, 00TS, 00TX, 00UT, 00VA, 00VI, 00W, 00WA, 00WI, 00WN, 00WV, 00WY, 00XS, 01A, 01AK, 01AL, 01AR, 01AZ, 01C, 01CA, 01CL, 01CN, 01CO, 01CT, 01FA, 01FD, 01FL, 01GA, 01GE, 01IA, 01ID, 01II, 01IL, 01IN, 01IS, 01J, 01K, 01KS, 01KY, 01LA, 01LL, 01LS, 01MA, 01MD, 01ME, 01MI, 01MN, 01MO, 01MT, 01NC, 01NE, 01NH, 01NJ, 01NM, 01NV, 01NY, 01OI, 01OK, 01OR, 01PA, 01PN, 01PS, 01SC, 01TA, 01TE, 01TN, 01TS, 01TX, 01U, 01UT, 01VA, 01WA, 01WI, 01WN, 01WT, 01WY, 01XA, 01XS, 02AK, 02..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idents = airports.select('ident).limit(200).distinct.as[String].collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "identSdf = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val identSdf = sdf.withColumn(\"ident\", shuffle(array(idents.map(lit(_)):_*))(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "identPqSink = org.apache.spark.sql.streaming.DataStreamWriter@75bdd6f9\n",
       "identPqSq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@78fbaec6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@78fbaec6"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val identPqSink = createParquetSink(identSdf, \"s2.parquet\")\n",
    "val identPqSq = identPqSink.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что данные записываются в `parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:31:54.019|2    |PL-0152|\n",
      "|2020-06-07 20:31:56.019|4    |FR-0254|\n",
      "|2020-06-07 20:32:00.019|8    |CZ-0107|\n",
      "|2020-06-07 20:32:03.019|11   |KR-0256|\n",
      "|2020-06-07 20:32:05.019|13   |KR-0306|\n",
      "+-----------------------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "identPq = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val identPq = spark.read.parquet(\"datasets/s2.parquet\")\n",
    "println(identPq.count)\n",
    "identPq.printSchema\n",
    "identPq.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Временно остановим стрим, он понадобится нам для следующих экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped FileStreamSource[file:/Users/t3nq/Projects/smz/de-spark-scala/datasets/s2.parquet]\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- `rate` - самый простой способ создать стрим для тестирования приложений\n",
    "- стрим начинает работу после вызова метода `start` и не блокирует основной поток программы\n",
    "- в одном Spark приложении может работать несколько стримов одновременно\n",
    "\n",
    "## File Streaming\n",
    "Spark позволяет запустить стрим, который будет \"слушать\" директорию и читать из нее новые файлы. При этом за раз будет прочитано количество файлов, установленное в параметре `maxFilesPerTrigger` [ссылка](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources). В этом кроется одна из основных проблем данного источника. Поскольку стрим, сконфигурированный под чтение небольших файлов, может \"упасть\", если в директорию начнут попадать файлы большого объема. Создадим стрим из директории `datasets/s2.parquet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalArgumentException\n",
       "Message: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:233)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:95)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:95)\n",
       "  at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:33)\n",
       "  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:215)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdfFromParquet = spark\n",
    "        .readStream\n",
    "        .format(\"parquet\")\n",
    "        .option(\"maxFilesPerTrigger\", \"1\")\n",
    "        .option(\"path\", \"datasets/s2.parquet\")\n",
    "        .load\n",
    "\n",
    "sdfFromParquet.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку в директорию могут попасть любые данные, а df должен иметь фиксированную схему, то Spark не позволяет нам создавать SDF на основе файлов без указания схемы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "letters = List(a, b, c, d, e, f, g, i)\n",
       "condition = (((((((startswith(ident, a) OR startswith(ident, b)) OR startswith(ident, c)) OR startswith(ident, d)) OR startswith(ident, e)) OR startswith(ident, f)) OR startswith(ident, g)) OR startswith(ident, i))\n",
       "sdfFromParquet = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val letters = List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"i\")\n",
    "val condition = letters.map { x => col(\"ident\").startsWith(x) }.reduce { (x,y) => x or y }\n",
    "\n",
    "val sdfFromParquet = spark\n",
    "        .readStream\n",
    "        .format(\"parquet\")\n",
    "        .schema(identPq.schema)\n",
    "        .option(\"maxFilesPerTrigger\", \"10\")\n",
    "        .option(\"path\", \"datasets/s2.parquet\")\n",
    "        .load\n",
    "        .withColumn(\"ident\", lower('ident))\n",
    "\n",
    "sdfFromParquet.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:31:54.019|2    |pl-0152|\n",
      "|2020-06-07 20:31:56.019|4    |fr-0254|\n",
      "|2020-06-07 20:32:00.019|8    |cz-0107|\n",
      "|2020-06-07 20:31:52.019|0    |sspl   |\n",
      "|2020-06-07 20:31:53.019|1    |vijn   |\n",
      "|2020-06-07 20:31:57.019|5    |la30   |\n",
      "|2020-06-07 20:31:58.019|6    |wi13   |\n",
      "|2020-06-07 20:31:59.019|7    |ku36   |\n",
      "|2020-06-07 20:31:55.019|3    |k13    |\n",
      "+-----------------------+-----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "consoleSink = org.apache.spark.sql.streaming.DataStreamWriter@dd89654\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5d484ca5"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:32:03.019|11   |kr-0256|\n",
      "|2020-06-07 20:32:05.019|13   |kr-0306|\n",
      "|2020-06-07 20:32:07.019|15   |py-0041|\n",
      "|2020-06-07 20:32:08.019|16   |us-0250|\n",
      "|2020-06-07 20:32:01.019|9    |30cl   |\n",
      "|2020-06-07 20:32:02.019|10   |29ll   |\n",
      "|2020-06-07 20:32:04.019|12   |kh68   |\n",
      "|2020-06-07 20:32:06.019|14   |zukj   |\n",
      "|2020-06-07 20:32:09.019|17   |pn29   |\n",
      "|2020-06-07 20:32:10.019|18   |sd99   |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:32:17.019|25   |fr-0387|\n",
      "|2020-06-07 20:32:18.019|26   |sk-0071|\n",
      "|2020-06-07 20:32:14.019|22   |ly-mra |\n",
      "|2020-06-07 20:32:11.019|19   |ssmu   |\n",
      "|2020-06-07 20:32:12.019|20   |my88   |\n",
      "|2020-06-07 20:32:13.019|21   |lehe   |\n",
      "|2020-06-07 20:32:15.019|23   |sswj   |\n",
      "|2020-06-07 20:32:16.019|24   |svse   |\n",
      "|2020-06-07 20:32:19.019|27   |mn86   |\n",
      "|2020-06-07 20:32:20.019|28   |kdew   |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:32:26.019|34   |hu-0018|\n",
      "|2020-06-07 20:32:28.019|36   |kr-0304|\n",
      "|2020-06-07 20:32:21.019|29   |khhg   |\n",
      "|2020-06-07 20:32:22.019|30   |gqna   |\n",
      "|2020-06-07 20:32:23.019|31   |krog   |\n",
      "|2020-06-07 20:32:24.019|32   |swas   |\n",
      "|2020-06-07 20:32:25.019|33   |siah   |\n",
      "|2020-06-07 20:32:27.019|35   |al10   |\n",
      "|2020-06-07 20:32:29.019|37   |mtch   |\n",
      "|2020-06-07 20:32:30.019|38   |sstq   |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:32:31.019|39   |tr-0020|\n",
      "|2020-06-07 20:32:34.019|42   |ar-0344|\n",
      "|2020-06-07 20:32:36.019|44   |kp-0089|\n",
      "|2020-06-07 20:32:32.019|40   |bgdh   |\n",
      "|2020-06-07 20:32:33.019|41   |wn82   |\n",
      "|2020-06-07 20:32:35.019|43   |kmaf   |\n",
      "|2020-06-07 20:32:37.019|45   |unok   |\n",
      "|2020-06-07 20:32:38.019|46   |fd35   |\n",
      "|2020-06-07 20:32:39.019|47   |m00    |\n",
      "|2020-06-07 20:32:40.019|48   |d81    |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:32:42.019|50   |ph-0069|\n",
      "|2020-06-07 20:32:44.019|52   |co-0042|\n",
      "|2020-06-07 20:32:46.019|54   |kr-0805|\n",
      "|2020-06-07 20:32:41.019|49   |k3h4   |\n",
      "|2020-06-07 20:32:45.019|53   |swzc   |\n",
      "|2020-06-07 20:32:47.019|55   |shhv   |\n",
      "|2020-06-07 20:32:48.019|56   |sjwt   |\n",
      "|2020-06-07 20:32:49.019|57   |8te8   |\n",
      "|2020-06-07 20:32:50.019|58   |bgum   |\n",
      "|2020-06-07 20:32:43.019|51   |ayy    |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:32:56.019|64   |it-0044|\n",
      "|2020-06-07 20:32:59.019|67   |ca-0626|\n",
      "|2020-06-07 20:32:51.019|59   |op12   |\n",
      "|2020-06-07 20:32:52.019|60   |al21   |\n",
      "|2020-06-07 20:32:53.019|61   |surv   |\n",
      "|2020-06-07 20:32:54.019|62   |cje4   |\n",
      "|2020-06-07 20:32:55.019|63   |ltcf   |\n",
      "|2020-06-07 20:32:57.019|65   |ku05   |\n",
      "|2020-06-07 20:32:58.019|66   |vake   |\n",
      "|2020-06-07 20:33:00.019|68   |74nj   |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:33:01.019|69   |cd-0016|\n",
      "|2020-06-07 20:33:05.019|73   |ar-0157|\n",
      "|2020-06-07 20:33:09.019|77   |ar-0328|\n",
      "|2020-06-07 20:33:02.019|70   |snrs   |\n",
      "|2020-06-07 20:33:03.019|71   |edad   |\n",
      "|2020-06-07 20:33:04.019|72   |my90   |\n",
      "|2020-06-07 20:33:06.019|74   |wmab   |\n",
      "|2020-06-07 20:33:07.019|75   |7ny0   |\n",
      "|2020-06-07 20:33:08.019|76   |klwm   |\n",
      "|2020-06-07 20:33:10.019|78   |1nk3   |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:33:15.019|83   |id-0115|\n",
      "|2020-06-07 20:33:20.019|88   |us-0290|\n",
      "|2020-06-07 20:33:12.019|80   |kt05   |\n",
      "|2020-06-07 20:33:13.019|81   |1nc2   |\n",
      "|2020-06-07 20:33:14.019|82   |2ca1   |\n",
      "|2020-06-07 20:33:16.019|84   |39tn   |\n",
      "|2020-06-07 20:33:17.019|85   |lrpt   |\n",
      "|2020-06-07 20:33:18.019|86   |lgsv   |\n",
      "|2020-06-07 20:33:19.019|87   |tffr   |\n",
      "|2020-06-07 20:33:11.019|79   |5o1    |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:33:21.019|89   |fr-0331|\n",
      "|2020-06-07 20:33:22.019|90   |no-0068|\n",
      "|2020-06-07 20:33:23.019|91   |ru-0476|\n",
      "|2020-06-07 20:33:29.019|97   |cn-0193|\n",
      "|2020-06-07 20:33:30.019|98   |kr-1021|\n",
      "|2020-06-07 20:33:24.019|92   |ne84   |\n",
      "|2020-06-07 20:33:25.019|93   |rpml   |\n",
      "|2020-06-07 20:33:26.019|94   |ykol   |\n",
      "|2020-06-07 20:33:27.019|95   |wn44   |\n",
      "|2020-06-07 20:33:28.019|96   |pa48   |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:33:34.019|102  |ec-0033|\n",
      "|2020-06-07 20:33:35.019|103  |br-0550|\n",
      "|2020-06-07 20:33:31.019|99   |sdsg   |\n",
      "|2020-06-07 20:33:32.019|100  |cnr9   |\n",
      "|2020-06-07 20:33:33.019|101  |1ta4   |\n",
      "|2020-06-07 20:33:36.019|104  |0al0   |\n",
      "|2020-06-07 20:33:37.019|105  |cjb7   |\n",
      "|2020-06-07 20:33:38.019|106  |83nj   |\n",
      "|2020-06-07 20:33:39.019|107  |sshx   |\n",
      "|2020-06-07 20:33:40.019|108  |swgn   |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:33:47.019|115  |gb-0132|\n",
      "|2020-06-07 20:33:41.019|109  |co46   |\n",
      "|2020-06-07 20:33:42.019|110  |kole   |\n",
      "|2020-06-07 20:33:43.019|111  |dbbp   |\n",
      "|2020-06-07 20:33:44.019|112  |ccd2   |\n",
      "|2020-06-07 20:33:45.019|113  |wrkb   |\n",
      "|2020-06-07 20:33:46.019|114  |siok   |\n",
      "|2020-06-07 20:33:48.019|116  |edos   |\n",
      "|2020-06-07 20:33:49.019|117  |fbbs   |\n",
      "|2020-06-07 20:33:50.019|118  |ta05   |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:33:57.019|125  |ar-0210|\n",
      "|2020-06-07 20:33:51.019|119  |ssbq   |\n",
      "|2020-06-07 20:33:52.019|120  |sifk   |\n",
      "|2020-06-07 20:33:53.019|121  |27oi   |\n",
      "|2020-06-07 20:33:54.019|122  |6fd9   |\n",
      "|2020-06-07 20:33:55.019|123  |8oh3   |\n",
      "|2020-06-07 20:33:56.019|124  |cycw   |\n",
      "|2020-06-07 20:33:58.019|126  |te90   |\n",
      "|2020-06-07 20:33:59.019|127  |00mt   |\n",
      "|2020-06-07 20:34:00.019|128  |00wn   |\n",
      "+-----------------------+-----+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-07 20:34:01.019|129  |02ne |\n",
      "|2020-06-07 20:34:02.019|130  |02fa |\n",
      "|2020-06-07 20:34:03.019|131  |01ak |\n",
      "|2020-06-07 20:34:04.019|132  |00ps |\n",
      "|2020-06-07 20:34:05.019|133  |00ls |\n",
      "|2020-06-07 20:34:06.019|134  |00ar |\n",
      "|2020-06-07 20:34:07.019|135  |01va |\n",
      "|2020-06-07 20:34:08.019|136  |03ak |\n",
      "|2020-06-07 20:34:09.019|137  |01pn |\n",
      "|2020-06-07 20:34:10.019|138  |01ok |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-07 20:34:11.019|139  |01ge |\n",
      "|2020-06-07 20:34:12.019|140  |01mt |\n",
      "|2020-06-07 20:34:13.019|141  |02ii |\n",
      "|2020-06-07 20:34:14.019|142  |01ak |\n",
      "|2020-06-07 20:34:15.019|143  |00ut |\n",
      "|2020-06-07 20:34:16.019|144  |01oi |\n",
      "|2020-06-07 20:34:17.019|145  |02cl |\n",
      "|2020-06-07 20:34:18.019|146  |03az |\n",
      "|2020-06-07 20:34:19.019|147  |01ky |\n",
      "|2020-06-07 20:34:20.019|148  |00al |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2020-06-07 20:34:21.019|149  |02ct |\n",
      "|2020-06-07 20:34:22.019|150  |03ak |\n",
      "|2020-06-07 20:34:23.019|151  |03fa |\n",
      "|2020-06-07 20:34:24.019|152  |03co |\n",
      "|2020-06-07 20:34:25.019|153  |02pa |\n",
      "|2020-06-07 20:34:26.019|154  |02kt |\n",
      "|2020-06-07 20:34:28.019|156  |02xs |\n",
      "|2020-06-07 20:34:29.019|157  |01or |\n",
      "|2020-06-07 20:34:30.019|158  |01wn |\n",
      "|2020-06-07 20:34:27.019|155  |00n  |\n",
      "+-----------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val consoleSink = createConsoleSink(sdfFromParquet)\n",
    "consoleSink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped FileStreamSource[file:/Users/t3nq/Projects/smz/de-spark-scala/datasets/s2.parquet]\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File source позволяет со всеми типами файлов, с которыми умеет работать Spark: `parquet`, `orc`, `csv`, `json`, `text`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Spark позволяет создавать SDF на базе всех поддерживаемых типов файлов\n",
    "- При создании SDF вы должны указать схему данных\n",
    "- File streaming имеет несколько серьезных недостатков:\n",
    "  + Входной поток можно ограничить только макисмальным количество файлов, попадающих в батч\n",
    "  + Если стрим упадает посередине файла, то при перезапуске эти данные будут обработаны еще раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"100\" height=\"100\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Apache_kafka.svg/1200px-Apache_kafka.svg.png\">\n",
    "\n",
    "## Kafka streaming\n",
    "\n",
    "https://kafka.apache.org\n",
    "\n",
    "**Apache Kafka** - самая распространенная в мире система, на основе которой строятся приложения для поточной обработки данных. Она имеет несколько преимуществ:\n",
    "- высокая пропускная способность\n",
    "- высокая доступность за счет распределенной архитектуры и репликации\n",
    "- у каждого сообщения есть свой номер, который называется offset, что позволяет гранулярно сохранять состояние стрима\n",
    "\n",
    "### Архитектура системы\n",
    "\n",
    "#### Topic\n",
    "Топик - это таблицы в Kafka. Мы пишем данные в топик и читаем данные из топика. Топик как правило распределен по нескольким узлам кластера для обеспечения высокой доступности и скорости работы с данными\n",
    "\n",
    "<img align=\"center\" width=\"500\" height=\"500\" src=\"https://kafka.apache.org/25/images/log_anatomy.png\">\n",
    "\n",
    "#### Partition\n",
    "Партиции - это блоки, из которых состоят топики. Партиция представляет собой неделимый блок, который хранится на одном из узлов. Топик может иметь произвольное количество партиций. Чем больше партиций - тем выше параллелзим при чтении и записи, однако слишком большое число партиций в топике может привести к замедлению работы всей системы.\n",
    "\n",
    "#### Replica\n",
    "Каждая партиция имеет (может иметь) несколько реплик. Внешние приложения всегда работают (читают и пишут) с основной репликой. Остальные реплики являются дочерними и не используются во внешнем IO. Если узел, на котором расположена основная реплика, падает, то одна из дочерних реплик становится основной и работа с данными продолжается\n",
    "\n",
    "#### Message\n",
    "Сообщения - это данные, которые мы пишем и читаем в Kafka. Они представлены кортежем (Key, Value), но ключ может быть иметь значение `null` (используется не всегда). Сереализация и десереализация данных всегда происходит на уровне клиентов Kafka. Сама Kafka ничего о типах данных не знает и хранит ключи и значения в виде массива байт\n",
    "\n",
    "#### Offset\n",
    "Оффсет - это порядковый номер сообщения в партиции. Когда мы пишем сообщение (сообщение всегда пишется в одну из партиций топика), Kafka помещает его в топик с номер `n+1`, где `n` - номер последнего сообщения в этом топике\n",
    "\n",
    "<img align=\"center\" width=\"400\" height=\"400\" src=\"https://kafka.apache.org/25/images/log_consumer.png\">\n",
    "\n",
    "#### Producer\n",
    "Producer - это приложение, которое пишет в топик. Producer'ов может быть много. Параллельная запись достигается за счет того, что каждое новое сообщение попадает в случайную партицию топика (если не указан `key`)\n",
    "\n",
    "#### Consumer\n",
    "Consumer - это приложение, читающее данные из топика. Consumer'ов может быть много, в этом случае они называются `consumer group`. Параллельное чтение достигается за счет распределения партиций топика между consumer'ами в рамках одной группы. Каждый consumer читает данные из \"своих\" партиций и ничего про другие не знает. Если consumer падает, то \"его\" партиции переходят другим consumer'ам.\n",
    "\n",
    "#### Commit\n",
    "Коммитом в Kafka называют сохранение информации о факте обработки сообщения с определенным оффсетом. Поскольку оффсеты для каждой партиции топика свои, то и информация о последнем обработанном оффсете хранится по каждой партиции отдельно. Обычные приложения пишут коммиты в специальный топик Kafka, который имеет название `__consumer_offsets`. Spark хранит обработанные оффсеты по каждому батчу в ФС (например, в HDFS).\n",
    "\n",
    "#### Retention\n",
    "Поскольку кластер Kafka не может хранить данные вечно, то в ее конфигурации задаются пороговые значение по **объему** и **времени хранения** для каждого топика, при превышении которых данные удаляются. Например, если у топика A установлен renention по времени 1 месяц, то данные будут хранится в системе не менее одного месяца (и затем будут удалены одной из внутренних подсистем)\n",
    "\n",
    "### Spark connector\n",
    "https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10  \n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html  \n",
    "\n",
    "### Запуск Kafka в docker\n",
    "```shell\n",
    "docker run --rm \\\n",
    "   -p 2181:2181 \\\n",
    "   --name=test_zoo \\\n",
    "   -e ZOOKEEPER_CLIENT_PORT=2181 \\\n",
    "   confluentinc/cp-zookeeper\n",
    "```\n",
    "\n",
    "```shell\n",
    "docker run --rm \\\n",
    "    -p 9092:9092 \\\n",
    "    --name=test_kafka \\\n",
    "    -e KAFKA_ZOOKEEPER_CONNECT=host.docker.internal:2181 \\\n",
    "    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\n",
    "    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\n",
    "    confluentinc/cp-kafka\n",
    "```\n",
    "\n",
    "### Работа с Kafka с помощь Static Dataframe\n",
    "\n",
    "Spark позволяет работать с кафкой как с обычной базой данных. Запишем данные в топик `test_topic0`. Для этого нам необходимо подготовить DF, в котором будет две колонки:\n",
    "- `value: String` - данные, которые мы хотим записать\n",
    "- `topic: String` - топик, куда писать каждую строку DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n",
      "+-----------------------+-----+-------+\n",
      "|timestamp              |value|ident  |\n",
      "+-----------------------+-----+-------+\n",
      "|2020-06-07 20:31:54.019|2    |PL-0152|\n",
      "|2020-06-07 20:31:56.019|4    |FR-0254|\n",
      "|2020-06-07 20:32:00.019|8    |CZ-0107|\n",
      "|2020-06-07 20:32:03.019|11   |KR-0256|\n",
      "|2020-06-07 20:32:05.019|13   |KR-0306|\n",
      "+-----------------------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "identPq = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val identPq = spark.read.parquet(\"datasets/s2.parquet\")\n",
    "println(identPq.count)\n",
    "identPq.printSchema\n",
    "identPq.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "writeKafka: [T](topic: String, data: org.apache.spark.sql.Dataset[T])Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def writeKafka[T](topic: String, data: Dataset[T]): Unit = {\n",
    "    val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"localhost:9092\"\n",
    "    )\n",
    "    \n",
    "    data.toJSON.withColumn(\"topic\", lit(topic)).write.format(\"kafka\").options(kafkaParams).save\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeKafka(\"test_topic0\", identPq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем данные из Kafka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     0|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     1|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     2|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     3|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     4|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     5|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     6|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     7|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     8|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|     9|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    10|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    11|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    12|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    13|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    14|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    15|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    16|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    17|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    18|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    19|2020-06-10 18:08:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> test_topic0)\n",
       "df = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"localhost:9092\",\n",
    "        \"subscribe\" -> \"test_topic0\"\n",
    "    )\n",
    "\n",
    "\n",
    "val df = spark.read.format(\"kafka\").options(kafkaParams).load\n",
    "\n",
    "df.printSchema\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение из Kafka имеет несколько особенностей:\n",
    "- по умолчанию читается все содержимое топика. Поскольку обычно в нем много данных, эта операция может создать большую нагрузку на кластер Kafka и Spark приложение\n",
    "- колонки `value` и `key` имеют тип `binary`, который необходимо десереализовать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы прочитать только определенную часть топика, нам необходимо задать минимальный и максимальный оффсет для чтения с помощью параметров `startingOffsets` , `endingOffsets`. Возьмем два случайных события:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------+\n",
      "|      topic|partition|offset|\n",
      "+-----------+---------+------+\n",
      "|test_topic0|        0|    20|\n",
      "|test_topic0|        0|    27|\n",
      "+-----------+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.sample(0.1).limit(2).select('topic, 'partition, 'offset).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основании этих событий подготовим параметры `startingOffsets` и `endingOffsets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    20|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    21|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    22|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    23|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    24|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    25|2020-06-10 18:08:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|    26|2020-06-10 18:08:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> test_topic0, startingOffsets -> \" { \"test_topic0\": { \"0\": 20 } } \", endingOffsets -> \" { \"test_topic0\": { \"0\": 27 } }  \")\n",
       "df = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"localhost:9092\",\n",
    "        \"subscribe\" -> \"test_topic0\",\n",
    "        \"startingOffsets\" -> \"\"\" { \"test_topic0\": { \"0\": 20 } } \"\"\",\n",
    "        \"endingOffsets\" -> \"\"\" { \"test_topic0\": { \"0\": 27 } }  \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "val df = spark.read.format(\"kafka\").options(kafkaParams).load\n",
    "\n",
    "df.printSchema\n",
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию параметр `startingOffsets` имеет значение `earliest`, а `endingOffsets` - `latest`. Поэтому, когда мы не указывали эти параметры, Spark прочитал содержимое всего топика\n",
    "\n",
    "Чтобы получить наши данные, которые мы записали в топик, нам необходимо их десереализовать. В нашем случае достаточно использовать `.cast(\"string\")`, однако это работает не всегда, т.к. формат данных может быть произвольным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|value                                                                    |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"timestamp\":\"2020-06-07T20:31:56.019+03:00\",\"value\":4,\"ident\":\"FR-0254\"}|\n",
      "|{\"timestamp\":\"2020-06-07T20:32:21.019+03:00\",\"value\":29,\"ident\":\"KHHG\"}  |\n",
      "|{\"timestamp\":\"2020-06-07T20:36:02.019+03:00\",\"value\":250,\"ident\":\"02MO\"} |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:06.019+03:00\",\"value\":194,\"ident\":\"00WY\"} |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:23.019+03:00\",\"value\":211,\"ident\":\"02IS\"} |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:42.019+03:00\",\"value\":230,\"ident\":\"00GE\"} |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:14.019+03:00\",\"value\":142,\"ident\":\"01AK\"} |\n",
      "+-------------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "+-------+-----------------------------+-----+\n",
      "|ident  |timestamp                    |value|\n",
      "+-------+-----------------------------+-----+\n",
      "|FR-0254|2020-06-07T20:31:56.019+03:00|4    |\n",
      "|KHHG   |2020-06-07T20:32:21.019+03:00|29   |\n",
      "|02MO   |2020-06-07T20:36:02.019+03:00|250  |\n",
      "|00WY   |2020-06-07T20:35:06.019+03:00|194  |\n",
      "|02IS   |2020-06-07T20:35:23.019+03:00|211  |\n",
      "|00GE   |2020-06-07T20:35:42.019+03:00|230  |\n",
      "|01AK   |2020-06-07T20:34:14.019+03:00|142  |\n",
      "+-------+-----------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jsonString = [value: string]\n",
       "parsed = [ident: string, timestamp: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, timestamp: string ... 1 more field]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonString = df.select('value.cast(\"string\")).as[String]\n",
    "\n",
    "jsonString.show(20, false)\n",
    "\n",
    "val parsed = spark.read.json(jsonString)\n",
    "parsed.printSchema\n",
    "parsed.show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с Kafka с помощью Streaming DF\n",
    "При создании SDF из Kafka необходимо помнить, что:\n",
    "- `startingOffsets` по умолчанию имеет значение `latest`\n",
    "- `endingOffsets` использовать нельзя\n",
    "- количество сообщений за батч можно (и нужно) ограничить параметром `maxOffsetPerTrigger` (по умолчанию он не задан и первый батч будет содержать данные всего топика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> test_topic0, startingOffsets -> earliest, maxOffsetsPerTrigger -> 5)\n",
       "sdf = [key: binary, value: binary ... 5 more fields]\n",
       "parsedSdf = [value: string, topic: string ... 2 more fields]\n",
       "sink = org.apache.spark.sql.streaming.DataStreamWriter@33bf26c8\n",
       "sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@65595976\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@65595976"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                    |topic      |partition|offset|\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:32:48.019+03:00\",\"value\":56,\"ident\":\"SJWT\"}  |test_topic0|0        |0     |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:05.019+03:00\",\"value\":193,\"ident\":\"02FA\"} |test_topic0|0        |1     |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:37.019+03:00\",\"value\":105,\"ident\":\"CJB7\"} |test_topic0|0        |2     |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:20.019+03:00\",\"value\":28,\"ident\":\"KDEW\"}  |test_topic0|0        |3     |\n",
      "|{\"timestamp\":\"2020-06-07T20:31:54.019+03:00\",\"value\":2,\"ident\":\"PL-0152\"}|test_topic0|0        |4     |\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:35:41.019+03:00\",\"value\":229,\"ident\":\"00LL\"}|test_topic0|0        |5     |\n",
      "|{\"timestamp\":\"2020-06-07T20:36:01.019+03:00\",\"value\":249,\"ident\":\"00WY\"}|test_topic0|0        |6     |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:22.019+03:00\",\"value\":210,\"ident\":\"01MO\"}|test_topic0|0        |7     |\n",
      "|{\"timestamp\":\"2020-06-07T20:31:52.019+03:00\",\"value\":0,\"ident\":\"SSPL\"}  |test_topic0|0        |8     |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:55.019+03:00\",\"value\":123,\"ident\":\"8OH3\"}|test_topic0|0        |9     |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:34:31.019+03:00\",\"value\":159,\"ident\":\"01KS\"}  |test_topic0|0        |10    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:13.019+03:00\",\"value\":141,\"ident\":\"02II\"}  |test_topic0|0        |11    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:56.019+03:00\",\"value\":64,\"ident\":\"IT-0044\"}|test_topic0|0        |12    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:48.019+03:00\",\"value\":176,\"ident\":\"00PS\"}  |test_topic0|0        |13    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:10.019+03:00\",\"value\":78,\"ident\":\"1NK3\"}   |test_topic0|0        |14    |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                  |topic      |partition|offset|\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:31:55.019+03:00\",\"value\":3,\"ident\":\"K13\"}  |test_topic0|0        |15    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:12.019+03:00\",\"value\":80,\"ident\":\"KT05\"}|test_topic0|0        |16    |\n",
      "|{\"timestamp\":\"2020-06-07T20:31:53.019+03:00\",\"value\":1,\"ident\":\"VIJN\"} |test_topic0|0        |17    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:39.019+03:00\",\"value\":47,\"ident\":\"M00\"} |test_topic0|0        |18    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:49.019+03:00\",\"value\":57,\"ident\":\"8TE8\"}|test_topic0|0        |19    |\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                    |topic      |partition|offset|\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:31:56.019+03:00\",\"value\":4,\"ident\":\"FR-0254\"}|test_topic0|0        |20    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:21.019+03:00\",\"value\":29,\"ident\":\"KHHG\"}  |test_topic0|0        |21    |\n",
      "|{\"timestamp\":\"2020-06-07T20:36:02.019+03:00\",\"value\":250,\"ident\":\"02MO\"} |test_topic0|0        |22    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:06.019+03:00\",\"value\":194,\"ident\":\"00WY\"} |test_topic0|0        |23    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:23.019+03:00\",\"value\":211,\"ident\":\"02IS\"} |test_topic0|0        |24    |\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:35:42.019+03:00\",\"value\":230,\"ident\":\"00GE\"}  |test_topic0|0        |25    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:14.019+03:00\",\"value\":142,\"ident\":\"01AK\"}  |test_topic0|0        |26    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:38.019+03:00\",\"value\":106,\"ident\":\"83NJ\"}  |test_topic0|0        |27    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:59.019+03:00\",\"value\":67,\"ident\":\"CA-0626\"}|test_topic0|0        |28    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:56.019+03:00\",\"value\":124,\"ident\":\"CYCW\"}  |test_topic0|0        |29    |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:34:32.019+03:00\",\"value\":160,\"ident\":\"01NM\"}|test_topic0|0        |30    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:49.019+03:00\",\"value\":177,\"ident\":\"02MI\"}|test_topic0|0        |31    |\n",
      "|{\"timestamp\":\"2020-06-07T20:31:57.019+03:00\",\"value\":5,\"ident\":\"LA30\"}  |test_topic0|0        |32    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:15.019+03:00\",\"value\":143,\"ident\":\"00UT\"}|test_topic0|0        |33    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:24.019+03:00\",\"value\":212,\"ident\":\"01MD\"}|test_topic0|0        |34    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:32:22.019+03:00\",\"value\":30,\"ident\":\"GQNA\"} |test_topic0|0        |35    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:50.019+03:00\",\"value\":178,\"ident\":\"02OR\"}|test_topic0|0        |36    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:43.019+03:00\",\"value\":231,\"ident\":\"00SD\"}|test_topic0|0        |37    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:50.019+03:00\",\"value\":58,\"ident\":\"BGUM\"} |test_topic0|0        |38    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:40.019+03:00\",\"value\":48,\"ident\":\"D81\"}  |test_topic0|0        |39    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                    |topic      |partition|offset|\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:35:07.019+03:00\",\"value\":195,\"ident\":\"02TS\"} |test_topic0|0        |40    |\n",
      "|{\"timestamp\":\"2020-06-07T20:36:03.019+03:00\",\"value\":251,\"ident\":\"02KY\"} |test_topic0|0        |41    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:33.019+03:00\",\"value\":161,\"ident\":\"01II\"} |test_topic0|0        |42    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:39.019+03:00\",\"value\":107,\"ident\":\"SSHX\"} |test_topic0|0        |43    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:00.019+03:00\",\"value\":8,\"ident\":\"CZ-0107\"}|test_topic0|0        |44    |\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:33:13.019+03:00\",\"value\":81,\"ident\":\"1NC2\"}   |test_topic0|0        |45    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:58.019+03:00\",\"value\":126,\"ident\":\"TE90\"}  |test_topic0|0        |46    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:51.019+03:00\",\"value\":179,\"ident\":\"03CA\"}  |test_topic0|0        |47    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:44.019+03:00\",\"value\":232,\"ident\":\"01TS\"}  |test_topic0|0        |48    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:01.019+03:00\",\"value\":69,\"ident\":\"CD-0016\"}|test_topic0|0        |49    |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:32:23.019+03:00\",\"value\":31,\"ident\":\"KROG\"} |test_topic0|0        |50    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:16.019+03:00\",\"value\":144,\"ident\":\"01OI\"}|test_topic0|0        |51    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:43.019+03:00\",\"value\":51,\"ident\":\"AYY\"}  |test_topic0|0        |52    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:25.019+03:00\",\"value\":213,\"ident\":\"02NJ\"}|test_topic0|0        |53    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:40.019+03:00\",\"value\":108,\"ident\":\"SWGN\"}|test_topic0|0        |54    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:35:08.019+03:00\",\"value\":196,\"ident\":\"00FA\"}|test_topic0|0        |55    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:51.019+03:00\",\"value\":59,\"ident\":\"OP12\"} |test_topic0|0        |56    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:34.019+03:00\",\"value\":162,\"ident\":\"00FD\"}|test_topic0|0        |57    |\n",
      "|{\"timestamp\":\"2020-06-07T20:31:58.019+03:00\",\"value\":6,\"ident\":\"WI13\"}  |test_topic0|0        |58    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:14.019+03:00\",\"value\":82,\"ident\":\"2CA1\"} |test_topic0|0        |59    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:36:04.019+03:00\",\"value\":252,\"ident\":\"02SC\"}  |test_topic0|0        |60    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:52.019+03:00\",\"value\":180,\"ident\":\"02OI\"}  |test_topic0|0        |61    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:59.019+03:00\",\"value\":127,\"ident\":\"00MT\"}  |test_topic0|0        |62    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:05.019+03:00\",\"value\":73,\"ident\":\"AR-0157\"}|test_topic0|0        |63    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:03.019+03:00\",\"value\":11,\"ident\":\"KR-0256\"}|test_topic0|0        |64    |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:32:24.019+03:00\",\"value\":32,\"ident\":\"SWAS\"} |test_topic0|0        |65    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:17.019+03:00\",\"value\":145,\"ident\":\"02CL\"}|test_topic0|0        |66    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:11.019+03:00\",\"value\":79,\"ident\":\"5O1\"}  |test_topic0|0        |67    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:41.019+03:00\",\"value\":109,\"ident\":\"CO46\"}|test_topic0|0        |68    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:26.019+03:00\",\"value\":214,\"ident\":\"00WI\"}|test_topic0|0        |69    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:35:45.019+03:00\",\"value\":233,\"ident\":\"01NC\"}|test_topic0|0        |70    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:52.019+03:00\",\"value\":60,\"ident\":\"AL21\"} |test_topic0|0        |71    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:35.019+03:00\",\"value\":163,\"ident\":\"03AR\"}|test_topic0|0        |72    |\n",
      "|{\"timestamp\":\"2020-06-07T20:31:59.019+03:00\",\"value\":7,\"ident\":\"KU36\"}  |test_topic0|0        |73    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:09.019+03:00\",\"value\":197,\"ident\":\"02II\"}|test_topic0|0        |74    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:35:27.019+03:00\",\"value\":215,\"ident\":\"02MT\"}  |test_topic0|0        |75    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:00.019+03:00\",\"value\":128,\"ident\":\"00WN\"}  |test_topic0|0        |76    |\n",
      "|{\"timestamp\":\"2020-06-07T20:36:05.019+03:00\",\"value\":253,\"ident\":\"03AZ\"}  |test_topic0|0        |77    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:05.019+03:00\",\"value\":13,\"ident\":\"KR-0306\"}|test_topic0|0        |78    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:36.019+03:00\",\"value\":164,\"ident\":\"00HI\"}  |test_topic0|0        |79    |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:32:53.019+03:00\",\"value\":61,\"ident\":\"SURV\"}   |test_topic0|0        |80    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:09.019+03:00\",\"value\":77,\"ident\":\"AR-0328\"}|test_topic0|0        |81    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:16.019+03:00\",\"value\":84,\"ident\":\"39TN\"}   |test_topic0|0        |82    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:25.019+03:00\",\"value\":33,\"ident\":\"SIAH\"}   |test_topic0|0        |83    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:42.019+03:00\",\"value\":110,\"ident\":\"KOLE\"}  |test_topic0|0        |84    |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:34:27.019+03:00\",\"value\":155,\"ident\":\"00N\"} |test_topic0|0        |85    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:18.019+03:00\",\"value\":146,\"ident\":\"03AZ\"}|test_topic0|0        |86    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:46.019+03:00\",\"value\":234,\"ident\":\"01ME\"}|test_topic0|0        |87    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:01.019+03:00\",\"value\":9,\"ident\":\"30CL\"}  |test_topic0|0        |88    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:53.019+03:00\",\"value\":181,\"ident\":\"01GA\"}|test_topic0|0        |89    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 18\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:36:06.019+03:00\",\"value\":254,\"ident\":\"02AZ\"}  |test_topic0|0        |90    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:01.019+03:00\",\"value\":129,\"ident\":\"02NE\"}  |test_topic0|0        |91    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:07.019+03:00\",\"value\":15,\"ident\":\"PY-0041\"}|test_topic0|0        |92    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:17.019+03:00\",\"value\":85,\"ident\":\"LRPT\"}   |test_topic0|0        |93    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:10.019+03:00\",\"value\":198,\"ident\":\"00LL\"}  |test_topic0|0        |94    |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 19\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:32:54.019+03:00\",\"value\":62,\"ident\":\"CJE4\"} |test_topic0|0        |95    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:31.019+03:00\",\"value\":219,\"ident\":\"00A\"} |test_topic0|0        |96    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:37.019+03:00\",\"value\":165,\"ident\":\"02VG\"}|test_topic0|0        |97    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:28.019+03:00\",\"value\":216,\"ident\":\"01AK\"}|test_topic0|0        |98    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:47.019+03:00\",\"value\":235,\"ident\":\"00MD\"}|test_topic0|0        |99    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 20\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:34:19.019+03:00\",\"value\":147,\"ident\":\"01KY\"}  |test_topic0|0        |100   |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:15.019+03:00\",\"value\":83,\"ident\":\"ID-0115\"}|test_topic0|0        |101   |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:43.019+03:00\",\"value\":111,\"ident\":\"DBBP\"}  |test_topic0|0        |102   |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:02.019+03:00\",\"value\":10,\"ident\":\"29LL\"}   |test_topic0|0        |103   |\n",
      "|{\"timestamp\":\"2020-06-07T20:36:07.019+03:00\",\"value\":255,\"ident\":\"01NE\"}  |test_topic0|0        |104   |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 21\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:32:08.019+03:00\",\"value\":16,\"ident\":\"US-0250\"}|test_topic0|0        |105   |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:02.019+03:00\",\"value\":130,\"ident\":\"02FA\"}  |test_topic0|0        |106   |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:55.019+03:00\",\"value\":63,\"ident\":\"LTCF\"}   |test_topic0|0        |107   |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:18.019+03:00\",\"value\":86,\"ident\":\"LGSV\"}   |test_topic0|0        |108   |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:27.019+03:00\",\"value\":35,\"ident\":\"AL10\"}   |test_topic0|0        |109   |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 22\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:35:40.019+03:00\",\"value\":228,\"ident\":\"02T\"}   |test_topic0|0        |110   |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:11.019+03:00\",\"value\":199,\"ident\":\"02XA\"}  |test_topic0|0        |111   |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:54.019+03:00\",\"value\":182,\"ident\":\"01NJ\"}  |test_topic0|0        |112   |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:20.019+03:00\",\"value\":88,\"ident\":\"US-0290\"}|test_topic0|0        |113   |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:50.019+03:00\",\"value\":238,\"ident\":\"01AR\"}  |test_topic0|0        |114   |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 23\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:32:17.019+03:00\",\"value\":25,\"ident\":\"FR-0387\"}|test_topic0|0        |115   |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:38.019+03:00\",\"value\":166,\"ident\":\"01KY\"}  |test_topic0|0        |116   |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:20.019+03:00\",\"value\":148,\"ident\":\"00AL\"}  |test_topic0|0        |117   |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:29.019+03:00\",\"value\":217,\"ident\":\"01IA\"}  |test_topic0|0        |118   |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:04.019+03:00\",\"value\":12,\"ident\":\"KH68\"}   |test_topic0|0        |119   |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 24\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:32:57.019+03:00\",\"value\":65,\"ident\":\"KU05\"} |test_topic0|0        |120   |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:12.019+03:00\",\"value\":200,\"ident\":\"02TS\"}|test_topic0|0        |121   |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:44.019+03:00\",\"value\":112,\"ident\":\"CCD2\"}|test_topic0|0        |122   |\n",
      "|{\"timestamp\":\"2020-06-07T20:36:08.019+03:00\",\"value\":256,\"ident\":\"00MO\"}|test_topic0|0        |123   |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:29.019+03:00\",\"value\":37,\"ident\":\"MTCH\"} |test_topic0|0        |124   |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 25\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:33:19.019+03:00\",\"value\":87,\"ident\":\"TFFR\"}   |test_topic0|0        |125   |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:21.019+03:00\",\"value\":89,\"ident\":\"FR-0331\"}|test_topic0|0        |126   |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:48.019+03:00\",\"value\":236,\"ident\":\"01U\"}   |test_topic0|0        |127   |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:03.019+03:00\",\"value\":131,\"ident\":\"01AK\"}  |test_topic0|0        |128   |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:39.019+03:00\",\"value\":167,\"ident\":\"01OK\"}  |test_topic0|0        |129   |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 26\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:35:51.019+03:00\",\"value\":239,\"ident\":\"00AA\"}  |test_topic0|0        |130   |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:18.019+03:00\",\"value\":26,\"ident\":\"SK-0071\"}|test_topic0|0        |131   |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:30.019+03:00\",\"value\":218,\"ident\":\"02SC\"}  |test_topic0|0        |132   |\n",
      "|{\"timestamp\":\"2020-06-07T20:36:09.019+03:00\",\"value\":257,\"ident\":\"00LA\"}  |test_topic0|0        |133   |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:49.019+03:00\",\"value\":237,\"ident\":\"01K\"}   |test_topic0|0        |134   |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 27\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:32:58.019+03:00\",\"value\":66,\"ident\":\"VAKE\"} |test_topic0|0        |135   |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:24.019+03:00\",\"value\":92,\"ident\":\"NE84\"} |test_topic0|0        |136   |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:21.019+03:00\",\"value\":149,\"ident\":\"02CT\"}|test_topic0|0        |137   |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:30.019+03:00\",\"value\":38,\"ident\":\"SSTQ\"} |test_topic0|0        |138   |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:45.019+03:00\",\"value\":113,\"ident\":\"WRKB\"}|test_topic0|0        |139   |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 28\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:33:22.019+03:00\",\"value\":90,\"ident\":\"NO-0068\"}|test_topic0|0        |140   |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:06.019+03:00\",\"value\":14,\"ident\":\"ZUKJ\"}   |test_topic0|0        |141   |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:40.019+03:00\",\"value\":168,\"ident\":\"00LL\"}  |test_topic0|0        |142   |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:55.019+03:00\",\"value\":183,\"ident\":\"00OH\"}  |test_topic0|0        |143   |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:13.019+03:00\",\"value\":201,\"ident\":\"01KS\"}  |test_topic0|0        |144   |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"localhost:9092\",\n",
    "        \"subscribe\" -> \"test_topic0\",\n",
    "        \"startingOffsets\" -> \"\"\"earliest\"\"\",\n",
    "        \"maxOffsetsPerTrigger\" -> \"5\"\n",
    "    )\n",
    "\n",
    "val sdf = spark.readStream.format(\"kafka\").options(kafkaParams).load\n",
    "val parsedSdf = sdf.select('value.cast(\"string\"), 'topic, 'partition, 'offset)\n",
    "\n",
    "val sink = createConsoleSink(parsedSdf)\n",
    "\n",
    "val sq = sink.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы перезапустим этот стрим, он повторно прочитает все данные. Чтобы обеспечить сохранение состояния стрима после обработки каждого батча, нам необходимо добавить параметр `checkpointLocation` в опции `writeStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSinkWithCheckpoint: (chkName: String, df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createConsoleSinkWithCheckpoint(chkName: String, df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"checkpointLocation\", s\"chk/$chkName\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sink = org.apache.spark.sql.streaming.DataStreamWriter@134d7092\n",
       "sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@63bc271f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@63bc271f"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                    |topic      |partition|offset|\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:31:56.019+03:00\",\"value\":4,\"ident\":\"FR-0254\"}|test_topic0|0        |20    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:21.019+03:00\",\"value\":29,\"ident\":\"KHHG\"}  |test_topic0|0        |21    |\n",
      "|{\"timestamp\":\"2020-06-07T20:36:02.019+03:00\",\"value\":250,\"ident\":\"02MO\"} |test_topic0|0        |22    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:06.019+03:00\",\"value\":194,\"ident\":\"00WY\"} |test_topic0|0        |23    |\n",
      "|{\"timestamp\":\"2020-06-07T20:35:23.019+03:00\",\"value\":211,\"ident\":\"02IS\"} |test_topic0|0        |24    |\n",
      "+-------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                     |topic      |partition|offset|\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2020-06-07T20:35:42.019+03:00\",\"value\":230,\"ident\":\"00GE\"}  |test_topic0|0        |25    |\n",
      "|{\"timestamp\":\"2020-06-07T20:34:14.019+03:00\",\"value\":142,\"ident\":\"01AK\"}  |test_topic0|0        |26    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:38.019+03:00\",\"value\":106,\"ident\":\"83NJ\"}  |test_topic0|0        |27    |\n",
      "|{\"timestamp\":\"2020-06-07T20:32:59.019+03:00\",\"value\":67,\"ident\":\"CA-0626\"}|test_topic0|0        |28    |\n",
      "|{\"timestamp\":\"2020-06-07T20:33:56.019+03:00\",\"value\":124,\"ident\":\"CYCW\"}  |test_topic0|0        |29    |\n",
      "+--------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sink = createConsoleSinkWithCheckpoint(\"test0\", parsedSdf)\n",
    "val sq = sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[test_topic0]]\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Apache Kafka - распределенная система, обеспечивающая передачу потока данных в слабосвязанных системах\n",
    "- Работать с Kafka можно как с использованием Static DF, так и с помощью Streaming DF\n",
    "- Чтобы стрим запоминал свое состояние после остановки, необходимо использовать checkpoint - директорию на HDFS (или локальной ФС), в которую будет сохранятся состояние стрима после каждого батча"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце работы не забудьте остановить Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
